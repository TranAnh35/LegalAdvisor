#!/usr/bin/env python3
"""
Version á»•n Ä‘á»‹nh cá»§a automatic_preprocess_vnlegaltext.py
- Checkpoint saving: LÆ°u tiáº¿n trÃ¬nh má»—i 100 files
- Memory management: Xá»­ lÃ½ theo batch nhá»
- Error recovery: Skip file lá»—i, tiáº¿p tá»¥c xá»­ lÃ½
- Resume capability: CÃ³ thá»ƒ tiáº¿p tá»¥c tá»« checkpoint
"""

import json
import re
import time
from pathlib import Path
from utils.paths import get_project_root
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import sqlite3
import argparse

# Sinh smart chunks trá»±c tiáº¿p khÃ´ng qua JSON
from tmp.export_chunks_storage import build_chunks_for_document

# Import PyVi
try:
    from pyvi import ViTokenizer
    HAS_PYVI = True
except ImportError:
    HAS_PYVI = False

class StableVNLegalTextProcessor:
    """Version á»•n Ä‘á»‹nh vá»›i checkpoint vÃ  error recovery"""

    def __init__(self, fast: bool = False):
        """Khá»Ÿi táº¡o processor"""
        self.fast = fast
        self.patterns = {
            'article': re.compile(r'^\s*Äiá»u\s+(\d+)\s*\.\s*(.*)$', re.MULTILINE),
            'clause': re.compile(r'^\s*(\d+)\s*\.\s+(.*)$', re.MULTILINE),
            'point': re.compile(r'^\s*\(?([a-z])\)\s+(.*)$', re.MULTILINE),
            'chapter': re.compile(r'^\s*CHÆ¯Æ NG\s+([IVXLCDM]+|[0-9]+)\b\s*(.*)$', re.MULTILINE | re.IGNORECASE),
            'section': re.compile(r'^\s*Má»¥c\s+(\d+)\b\s*(.*)$', re.MULTILINE | re.IGNORECASE)
        }

        # Mapping sá»‘ La MÃ£
        self.roman_numerals = {
            'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
            'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,
            'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15
        }

        # Táº£i stopwords - Báº®T BUá»˜C
        self.stopwords = self._load_stopwords()

        # Kiá»ƒm tra cÃ¡c dependencies báº¯t buá»™c
        if not HAS_PYVI:
            raise ImportError("âŒ PyVi is required but not available. Please install pyvi: pip install pyvi")

        if not self.stopwords:
            raise ValueError("âŒ Stopwords file is required but not available or empty")

    def _load_stopwords(self) -> set:
        """Táº£i stopwords tá»« file"""
        try:
            stopwords_file = Path(__file__).parent.parent / "data" / "vietnamese-stopwords-dash.txt"
            if stopwords_file.exists():
                with open(stopwords_file, 'r', encoding='utf-8') as f:
                    stopwords = set(line.strip().lower() for line in f if line.strip())
                print(f"âœ… ÄÃ£ táº£i {len(stopwords)} stopwords")
                return stopwords
            else:
                print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y file stopwords")
                return set()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi táº£i stopwords: {e}")
            return set()

    def _load_checkpoint(self, checkpoint_file: Path) -> Dict[str, Any]:
        """Táº£i checkpoint Ä‘á»ƒ resume"""
        if checkpoint_file.exists():
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ táº£i checkpoint: {e}")
        return {'processed_files': [], 'last_index': 0, 'start_time': time.time()}

    def _save_checkpoint(self, checkpoint_file: Path, processed_data: List[Dict], last_index: int, start_time: float):
        """LÆ°u checkpoint"""
        try:
            checkpoint = {
                'processed_files': processed_data,
                'last_index': last_index,
                'start_time': start_time,
                'timestamp': time.time()
            }
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi lÆ°u checkpoint: {e}")

    def clean_text_with_vietnamese_support(self, text: str, remove_stopwords: bool = False, keep_underscore: bool = True) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n vá»›i PyVi.

        - remove_stopwords: True Ä‘á»ƒ loáº¡i bá» stopwords (dÃ¹ng cho UI), False cho index
        - keep_underscore: True Ä‘á»ƒ giá»¯ "_" cá»§a ViTokenizer (dÃ¹ng cho index)
        """
        if not text:
            return ""

        try:
            # Chuáº©n hÃ³a xuá»‘ng dÃ²ng Windows vá» \n
            text = text.replace('\r\n', '\n').replace('\r', '\n')

            # XÃ³a cÃ¡c tag XML nhÆ°ng cá»‘ gáº¯ng giá»¯ cáº¥u trÃºc dÃ²ng (thay báº±ng xuá»‘ng dÃ²ng náº¿u cÃ³ khá»‘i lá»›n)
            text = re.sub(r'<[^>]+>', ' ', text)

            # Chuáº©n hÃ³a khoáº£ng tráº¯ng nhÆ°ng GIá»® láº¡i dáº¥u xuá»‘ng dÃ²ng
            text = re.sub(r'[ \t]+', ' ', text)
            # ChÃ¨n khoáº£ng tráº¯ng trÆ°á»›c khi tokenize Ä‘á»ƒ trÃ¡nh dÃ­nh tá»« cáº¡nh dáº¥u cÃ¢u (khÃ´ng áº£nh hÆ°á»Ÿng \n)
            text = re.sub(r'([,;:\.!?)(\[\]"\'])', r' \1 ', text)
            # KHÃ”NG gá»™p \n: bá» dÃ²ng sau Ä‘á»ƒ báº£o toÃ n cáº¥u trÃºc
            # text = re.sub(r'\s+', ' ', text)

            # Sá»­ dá»¥ng pyvi Ä‘á»ƒ tÃ¡ch tá»« tá»± Ä‘á»™ng - Báº®T BUá»˜C
            text = ViTokenizer.tokenize(text)

            # Loáº¡i bá» stopwords náº¿u Ä‘Æ°á»£c yÃªu cáº§u
            if remove_stopwords:
                words = text.split()
                words = [word for word in words if word.lower() not in self.stopwords]
                text = ' '.join(words)

            # Xá»­ lÃ½ dáº¥u gáº¡ch dÆ°á»›i tá»« ViTokenizer
            if not keep_underscore:
                text = text.replace('_', ' ')

            # LÃ m sáº¡ch nháº¹ nhÃ ng - GIá»® Láº I Dáº¤U TIáº¾NG VIá»†T VÃ€ KÃ HIá»†U PHá»” BIáº¾N
            pattern = r"[^a-zA-Z0-9\sÃ€-á»¹Ã -á»¹\.,;:!\?()\[\]\"'\-\n_â€“â€”â€¦/Â§â€œâ€â€˜â€™]"
            text = re.sub(pattern, '', text)

            return text.strip()

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi xá»­ lÃ½ text: {e}")
            return text[:500] if text else ""  # Return truncated text as fallback

    def _normalize_underscore_separated_words(self, text: str) -> str:
        """Deprecated: KhÃ´ng sá»­ dá»¥ng trong pipeline chÃ­nh."""
        return text

    def parse_legal_structure(self, content: str, document_title: str = "") -> List[Dict[str, Any]]:
        """Parse cáº¥u trÃºc phÃ¡p lÃ½ (CHÆ¯Æ NG â†’ Má»¤C â†’ Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm)."""
        sections: List[Dict[str, Any]] = []

        try:
            lines = content.split('\n')

            current_chapter: Optional[Dict[str, Any]] = None
            current_section: Optional[Dict[str, Any]] = None
            current_article: Optional[Dict[str, Any]] = None
            current_clause: Optional[Dict[str, Any]] = None

            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue

                # CHÆ¯Æ NG
                chapter_match = self.patterns['chapter'].search(line)
                if chapter_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)
                        current_article = None
                    if current_section:
                        sections.append(current_section)
                        current_section = None
                    if current_chapter:
                        sections.append(current_chapter)

                    chap_num = chapter_match.group(1)
                    chap_title = chapter_match.group(2).strip() if chapter_match.lastindex and chapter_match.group(2) else ""
                    current_chapter = {
                        'title': f"CHÆ¯Æ NG {chap_num}",
                        'heading': chap_title,
                        'content': "",
                        'section_type': 'chapter',
                        'section_number': chap_num,
                        'level': 0
                    }
                    continue

                # Má»¤C
                section_match = self.patterns['section'].search(line)
                if section_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)
                        current_article = None
                    if current_section:
                        sections.append(current_section)

                    sec_num = section_match.group(1)
                    sec_title = section_match.group(2).strip() if section_match.lastindex and section_match.group(2) else ""
                    current_section = {
                        'title': f"Má»¥c {sec_num}",
                        'heading': sec_title,
                        'content': "",
                        'section_type': 'section',
                        'section_number': sec_num,
                        'parent_section': current_chapter['title'] if current_chapter else None,
                        'level': 0.5
                    }
                    continue

                # Äiá»u
                article_match = self.patterns['article'].search(line)
                if article_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)

                    article_num = article_match.group(1)
                    article_heading = article_match.group(2).strip() if article_match.lastindex and article_match.group(2) else ""
                    current_article = {
                        'title': f"Äiá»u {article_num}",
                        'heading': article_heading,
                        'content': "",
                        'section_type': 'article',
                        'section_number': article_num,
                        'parent_section': current_section['title'] if current_section else (current_chapter['title'] if current_chapter else None),
                        'level': 1
                    }
                    continue

                # Khoáº£n
                clause_match = self.patterns['clause'].search(line)
                if clause_match and current_article:
                    if current_clause:
                        sections.append(current_clause)

                    clause_num = clause_match.group(1)
                    clause_content = clause_match.group(2).strip()
                    current_clause = {
                        'title': f"Khoáº£n {clause_num}",
                        'content': clause_content,
                        'section_type': 'clause',
                        'section_number': clause_num,
                        'parent_section': current_article['title'],
                        'level': 2
                    }
                    continue

                # Äiá»ƒm
                point_match = self.patterns['point'].search(line)
                if point_match and current_clause:
                    point_label = point_match.group(1)
                    point_content = point_match.group(2).strip()
                    sections.append({
                        'title': f"Äiá»ƒm {point_label})",
                        'content': point_content,
                        'section_type': 'point',
                        'section_number': point_label,
                        'parent_section': current_clause['title'],
                        'level': 3
                    })
                    continue

                # Ná»™i dung
                if current_clause:
                    current_clause['content'] = (current_clause['content'] + ' ' + line).strip() if current_clause.get('content') else line
                elif current_article:
                    current_article['content'] = (current_article['content'] + ' ' + line).strip() if current_article.get('content') else line
                elif current_section:
                    current_section['content'] = (current_section['content'] + ' ' + line).strip() if current_section.get('content') else line
                elif current_chapter:
                    current_chapter['content'] = (current_chapter['content'] + ' ' + line).strip() if current_chapter.get('content') else line

            # Flush cuá»‘i cÃ¹ng
            if current_clause:
                sections.append(current_clause)
            if current_article:
                sections.append(current_article)
            if current_section:
                sections.append(current_section)
            if current_chapter:
                sections.append(current_chapter)

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi parse structure: {e}")

        return sections

    def extract_title_and_metadata(self, content: str, raw_text: Optional[str] = None) -> Dict[str, Any]:
        """TrÃ­ch xuáº¥t tiÃªu Ä‘á» vÃ  metadata cÆ¡ báº£n (heuristic)."""
        try:
            lines = content.split('\n')[:10]  # Chá»‰ xem pháº§n Ä‘áº§u

            metadata = {
                'title': 'Unknown',
                'document_type': 'legal_document',
                'language': 'vietnamese'
            }

            # Æ¯u tiÃªn dÃ²ng Ä‘áº§u khÃ´ng rá»—ng lÃ m tiÃªu Ä‘á» náº¿u chá»©a tá»« khÃ³a phÃ¡p lÃ½
            for line in lines:
                line = line.strip()
                if not line or line.startswith('<'):
                    continue
                low = line.lower()
                if any(k in low for k in ['luáº­t', 'nghá»‹ Ä‘á»‹nh', 'thÃ´ng tÆ°', 'quyáº¿t Ä‘á»‹nh', 'hiáº¿n phÃ¡p']):
                    metadata['title'] = line
                    break

            # Heuristic tá»« raw_text cho cÃ¡c trÆ°á»ng phá»• biáº¿n
            if raw_text:
                head = '\n'.join(raw_text.splitlines()[:120])
                patterns = {
                    'reference_number': r'\bSá»‘[:\s]+([^\n]+)',
                    'issuing_body': r'\bCÆ¡\s*quan\s*ban\s*hÃ nh[:\s]+([^\n]+)',
                    'promulgation_date': r'\bNgÃ y\s*ban\s*hÃ nh[:\s]+([^\n]+)',
                    'effective_date': r'\bNgÃ y\s*cÃ³\s*hiá»‡u\s*lá»±c[:\s]+([^\n]+)',
                    'status': r'\bTÃ¬nh\s*tráº¡ng[:\s]+([^\n]+)'
                }
                for key, pat in patterns.items():
                    m = re.search(pat, head, flags=re.IGNORECASE)
                    if m:
                        metadata[key] = m.group(1).strip()

            return metadata

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi extract metadata: {e}")
            return {'title': 'Error', 'document_type': 'unknown', 'language': 'vietnamese'}

    def process_single_file(self, file_path: Path) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t file XML (version á»•n Ä‘á»‹nh)"""
        try:
            # Äá»c file vá»›i error handling
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_content = f.read()
            except UnicodeDecodeError:
                # Thá»­ encoding khÃ¡c
                with open(file_path, 'r', encoding='utf-8-sig') as f:
                    raw_content = f.read()
            except Exception as e:
                return {
                    'file_name': file_path.name,
                    'file_path': str(file_path),
                    'error': f"Cannot read file: {e}",
                    'error_type': 'file_read_error'
                }

            # Giá»›i háº¡n kÃ­ch thÆ°á»›c file (trÃ¡nh memory issue)
            if len(raw_content) > 10 * 1024 * 1024:  # 10MB
                return {
                    'file_name': file_path.name,
                    'file_path': str(file_path),
                    'error': "File too large (>10MB)",
                    'error_type': 'file_too_large'
                }

            # LÃ m sáº¡ch vÄƒn báº£n (hai báº£n: cho index vÃ  cho UI)
            cleaned_for_index = self.clean_text_with_vietnamese_support(
                raw_content,
                remove_stopwords=False,
                keep_underscore=True
            )
            cleaned_for_ui = None
            if not self.fast:
                cleaned_for_ui = self.clean_text_with_vietnamese_support(
                    raw_content,
                    remove_stopwords=True,
                    keep_underscore=False
                )

            # TrÃ­ch xuáº¥t metadata
            metadata = self.extract_title_and_metadata(cleaned_for_index, raw_text=raw_content)

            # Parse cáº¥u trÃºc phÃ¡p lÃ½
            document_title = metadata.get('title', file_path.name.replace('.xml', ''))
            sections = self.parse_legal_structure(cleaned_for_index, document_title)

            # Táº¡o káº¿t quáº£
            result = {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'metadata': metadata,
                'cleaned_content': cleaned_for_index,
                'cleaned_content_index': cleaned_for_index,
                'cleaned_content_ui': cleaned_for_ui if cleaned_for_ui is not None else '',
                'sections': sections,
                'stats': {} if self.fast else {
                    'total_sections': len(sections),
                    'articles_count': len([s for s in sections if s['section_type'] == 'article']),
                    'clauses_count': len([s for s in sections if s['section_type'] == 'clause']),
                    'points_count': len([s for s in sections if s['section_type'] == 'point']),
                    'total_words': len(cleaned_for_index.split())
                },
                'processing_info': {
                    'used_pyvi': True,
                    'stopwords_removed_for_index': False,
                    'stopwords_removed_for_ui': (cleaned_for_ui is not None),
                    'processing_time': time.time(),
                    'file_size': len(raw_content)
                }
            }

            return result

        except Exception as e:
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'error': str(e),
                'error_type': type(e).__name__
            }

    def process_record(self, record: Dict[str, Any], record_id: Optional[str] = None) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t record tá»« JSON/JSONL/Parquet (GreenNode corpus).

        Gáº¯ng map cÃ¡c trÆ°á»ng phá»• biáº¿n nhÆ° 'text', 'content', 'body' vÃ o raw_content.
        record_id dÃ¹ng lÃ m file_name giáº£ Ä‘á»ƒ theo dÃµi nguá»“n.
        """
        try:
            # Láº¥y raw text tá»« cÃ¡c trÆ°á»ng phá»• biáº¿n
            raw_content = None
            for key in ('text', 'content', 'body', 'passage', 'document'):
                if key in record and record.get(key):
                    raw_content = record.get(key)
                    break

            # Náº¿u khÃ´ng cÃ³, thá»­ ghÃ©p cÃ¡c trÆ°á»ng mÃ´ táº£
            if raw_content is None:
                candidates = []
                for k, v in record.items():
                    if isinstance(v, str) and len(v) > 50:
                        candidates.append(v)
                raw_content = '\n'.join(candidates) if candidates else ''

            raw_content = raw_content or ''

            # Giá»›i háº¡n kÃ­ch thÆ°á»›c record Ä‘á»ƒ trÃ¡nh OOM
            if len(raw_content) > 10 * 1024 * 1024:
                return {
                    'file_name': record_id or 'record',
                    'file_path': record_id or 'record',
                    'error': 'Record too large (>10MB)',
                    'error_type': 'record_too_large'
                }

            cleaned_for_index = self.clean_text_with_vietnamese_support(
                raw_content,
                remove_stopwords=False,
                keep_underscore=True
            )
            cleaned_for_ui = None
            if not self.fast:
                cleaned_for_ui = self.clean_text_with_vietnamese_support(
                    raw_content,
                    remove_stopwords=True,
                    keep_underscore=False
                )

            metadata = self.extract_title_and_metadata(cleaned_for_index, raw_text=raw_content)
            document_title = metadata.get('title', record_id or 'record')
            sections = self.parse_legal_structure(cleaned_for_index, document_title)

            result = {
                'file_name': record_id or str(record.get('id') or 'record'),
                'file_path': str(record.get('id') or record_id or 'record'),
                'metadata': metadata,
                'cleaned_content': cleaned_for_index,
                'cleaned_content_index': cleaned_for_index,
                'cleaned_content_ui': cleaned_for_ui if cleaned_for_ui is not None else '',
                'sections': sections,
                'stats': {} if self.fast else {
                    'total_sections': len(sections),
                    'articles_count': len([s for s in sections if s['section_type'] == 'article']),
                    'clauses_count': len([s for s in sections if s['section_type'] == 'clause']),
                    'points_count': len([s for s in sections if s['section_type'] == 'point']),
                    'total_words': len(cleaned_for_index.split())
                },
                'processing_info': {
                    'used_pyvi': True,
                    'stopwords_removed_for_index': False,
                    'stopwords_removed_for_ui': (cleaned_for_ui is not None),
                    'processing_time': time.time(),
                    'record_size': len(raw_content)
                }
            }

            return result
        except Exception as e:
            return {
                'file_name': record_id or str(record.get('id') or 'record'),
                'file_path': record_id or str(record.get('id') or 'record'),
                'error': str(e),
                'error_type': type(e).__name__
            }

    def process_all_files_stable(self, input_dir: Path, output_path: Path, batch_size: int = 50, skip_parquet: bool = False):
        """Xá»­ lÃ½ táº¥t cáº£ files vÃ  xuáº¥t tháº³ng smart chunks vÃ o SQLite/Parquet, khÃ´ng táº¡o JSON tá»•ng."""
        # Táº¡o thÆ° má»¥c output
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ÄÃ­ch xuáº¥t tá»‘i Æ°u
        processed_dir = output_path.parent
        db_path = processed_dir / 'smart_chunks_stable.db'
        parquet_path = processed_dir / 'smart_chunks_stable.parquet'

        # XÃ³a output cÅ© Ä‘á»ƒ luÃ´n táº¡o DB/Parquet má»›i (trÃ¡nh phÃ¬nh file khi reprocess)
        try:
            if db_path.exists():
                db_path.unlink()
            if parquet_path.exists():
                parquet_path.unlink()
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ xÃ³a output cÅ©: {e}. Sáº½ ghi Ä‘Ã¨ báº£ng thay tháº¿.")

        # Khá»Ÿi táº¡o SQLite (táº¡o má»›i báº£ng)
        conn = sqlite3.connect(str(db_path))
        cur = conn.cursor()
        # PRAGMA tá»‘i Æ°u ghi nhanh (Ä‘Ã¡nh Ä‘á»•i Ä‘á»™ bá»n trong phiÃªn xá»­ lÃ½ batch)
        try:
            cur.execute("PRAGMA journal_mode=OFF;")
            cur.execute("PRAGMA synchronous=OFF;")
            cur.execute("PRAGMA temp_store=MEMORY;")
            cur.execute("PRAGMA cache_size=-200000;")
        except Exception:
            pass
        cur.execute('DROP TABLE IF EXISTS chunks')
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS chunks (
                chunk_id      INTEGER PRIMARY KEY,
                doc_file      TEXT,
                doc_title     TEXT,
                chapter       TEXT,
                section       TEXT,
                article       TEXT,
                article_heading TEXT,
                clause        TEXT,
                point         TEXT,
                chunk_index   INTEGER,
                content       TEXT,
                word_count    INTEGER,
                chunk_type    TEXT
            )
            """
        )

        def to_int(value):
            if value is None:
                return None
            try:
                return int(value)
            except Exception:
                return None

        def to_text(value):
            if value is None:
                return None
            try:
                return str(value)
            except Exception:
                return None

        total_chunks = 0

        # Há»— trá»£ Ä‘áº§u vÃ o lÃ  thÆ° má»¥c chá»©a XML hoáº·c má»™t file JSONL/Parquet táº­p há»£p documents
        xml_files = []
        is_jsonl_input = False
        if input_dir.is_file():
            # Náº¿u lÃ  JSONL (vÃ­ dá»¥ GreenNode corpus.jsonl) hoáº·c Parquet
            if input_dir.suffix.lower() in ('.jsonl', '.json'):
                is_jsonl_input = True
                print(f"ğŸ“ Input lÃ  file JSONL: {input_dir}")
            elif input_dir.suffix.lower() in ('.parquet', '.pq'):
                is_jsonl_input = True
                print(f"ğŸ“ Input lÃ  file Parquet: {input_dir}")
            else:
                print(f"âŒ Input file khÃ´ng há»— trá»£: {input_dir}")
                conn.close()
                return
        else:
            xml_files = list(input_dir.glob("*.xml"))

        if not is_jsonl_input and not xml_files:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file XML nÃ o trong {input_dir}")
            # ÄÃ³ng káº¿t ná»‘i náº¿u Ä‘Ã£ má»Ÿ
            conn.close()
            return

        if not is_jsonl_input:
            print(f"ğŸ“ TÃ¬m tháº¥y {len(xml_files)} file XML")
        print(f"ğŸ¤– Sá»­ dá»¥ng xá»­ lÃ½ á»•n Ä‘á»‹nh: PyVi {'CÃ³' if HAS_PYVI else 'KhÃ´ng'}")
        print(f"ğŸ“Š Batch size: {batch_size} files")

        # KhÃ´ng sá»­ dá»¥ng JSON checkpoint lá»›n; theo dÃµi tá»‘i thiá»ƒu báº±ng biáº¿n Ä‘áº¿m
        start_time = time.time()
        total_processed = 0

        remaining_files = xml_files
        # Náº¿u input lÃ  JSONL/Parquet, xá»­ lÃ½ theo luá»“ng Ä‘á»c tá»«ng record
        if is_jsonl_input:
            # Äá»c file JSONL tá»«ng dÃ²ng (nháº¹ bá»™ nhá»›)
            print(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ JSONL/Parquet: {input_dir}")
            pbar = None
            try:
                if input_dir.suffix.lower() in ('.parquet', '.pq'):
                    import pandas as pd  # type: ignore
                    df = pd.read_parquet(input_dir)
                    total = len(df)
                    from tqdm import tqdm as _tqdm
                    for idx, row in _tqdm(df.iterrows(), total=total, desc="Processing records"):
                        try:
                            record = dict(row)
                            # Compose a pseudo file name
                            file_name = str(record.get('id') or record.get('doc_id') or f"rec_{idx}")
                            result = self.process_record(record, record_id=file_name)
                            total_processed += 1
                            if 'error' in result:
                                continue
                            # For corpus records: create exactly ONE chunk per record (do not further split)
                            rows = []
                            # Normalize content: prefer cleaned_for_index
                            _content = to_text(result.get('cleaned_content_index') or result.get('cleaned_content') or '')
                            content = ' '.join(_content.split()) if _content else None
                            doc_file = to_text(result.get('file_name'))
                            _title = to_text((result.get('metadata') or {}).get('title'))
                            doc_title = (_title[:200] if _title and len(_title) > 200 else _title)
                            chunk_id = total_chunks
                            chunk_index = 0
                            word_count = len(content.split()) if content else 0
                            chunk_type = 'corpus_record'
                            rows.append((
                                to_int(chunk_id),
                                doc_file,
                                doc_title,
                                None,
                                None,
                                None,
                                None,
                                None,
                                None,
                                chunk_index,
                                content,
                                to_int(word_count),
                                chunk_type,
                            ))
                            if rows:
                                cur.executemany(
                                    'INSERT INTO chunks (chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)',
                                    rows
                                )
                                total_chunks += len(rows)
                                if total_chunks % 5000 == 0:
                                    conn.commit()
                        except Exception as e:
                            print(f"âŒ Lá»—i xá»­ lÃ½ record {idx}: {e}")
                    conn.commit()
                else:
                    # JSONL
                    with open(input_dir, 'r', encoding='utf-8') as jf:
                        from tqdm import tqdm as _tqdm
                        # Æ¯á»›c tÃ­nh tá»•ng dÃ²ng khÃ´ng kháº£ thi nhanh, dÃ¹ng tqdm Ä‘Æ¡n giáº£n
                        for idx, line in enumerate(_tqdm(jf, desc="Processing JSONL lines")):
                            try:
                                import json as _json
                                record = _json.loads(line)
                                file_name = str(record.get('id') or record.get('doc_id') or f"rec_{idx}")
                                result = self.process_record(record, record_id=file_name)
                                total_processed += 1
                                if 'error' in result:
                                    continue
                                # For corpus records: create exactly ONE chunk per record (do not further split)
                                rows = []
                                _content = to_text(result.get('cleaned_content_index') or result.get('cleaned_content') or '')
                                content = ' '.join(_content.split()) if _content else None
                                doc_file = to_text(result.get('file_name'))
                                _title = to_text((result.get('metadata') or {}).get('title'))
                                doc_title = (_title[:200] if _title and len(_title) > 200 else _title)
                                chunk_id = total_chunks
                                chunk_index = 0
                                word_count = len(content.split()) if content else 0
                                chunk_type = 'corpus_record'
                                rows.append((
                                    to_int(chunk_id),
                                    doc_file,
                                    doc_title,
                                    None,
                                    None,
                                    None,
                                    None,
                                    None,
                                    None,
                                    chunk_index,
                                    content,
                                    to_int(word_count),
                                    chunk_type,
                                ))
                                if rows:
                                    cur.executemany(
                                        'INSERT INTO chunks (chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)',
                                        rows
                                    )
                                    total_chunks += len(rows)
                                    if total_chunks % 5000 == 0:
                                        conn.commit()
                            except Exception as e:
                                print(f"âŒ Lá»—i xá»­ lÃ½ JSONL dÃ²ng {idx}: {e}")
                    conn.commit()
            finally:
                print(f"ğŸ’¾ ÄÃ£ commit, tá»•ng processed: {total_processed}, tá»•ng chunks: {total_chunks}")
        else:
            remaining_files = xml_files
            print(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ {len(xml_files)} files")

            if not remaining_files:
                print("âœ… Táº¥t cáº£ files Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½!")
                return

            # Xá»­ lÃ½ theo batch
            # Báº¯t Ä‘áº§u Ä‘áº¿m tá»« 0
            # total_processed Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o á»Ÿ trÃªn

            with tqdm(total=len(remaining_files), desc="Processing XML files", unit="file") as pbar:
                for i in range(0, len(remaining_files), batch_size):
                    batch = remaining_files[i:i + batch_size]

                    for xml_file in batch:
                        try:
                            result = self.process_single_file(xml_file)

                            # Cáº­p nháº­t progress
                            total_processed += 1
                            pbar.update(1)

                            # Bá» qua file lá»—i
                            if 'error' in result:
                                continue

                            # XÃ¢y smart chunks cho tÃ i liá»‡u nÃ y
                            doc_chunks = build_chunks_for_document(result, start_chunk_id=total_chunks)

                            # Chuáº©n hÃ³a vÃ  chÃ¨n vÃ o SQLite
                            rows = []
                            for ch in doc_chunks:
                                chunk_id = to_int(ch.get('chunk_id'))
                                doc_file = to_text(ch.get('doc_file'))
                                # RÃºt gá»n doc_title Ä‘á»ƒ giáº£m size: cáº¯t tá»‘i Ä‘a 200 kÃ½ tá»±
                                _title = to_text(ch.get('doc_title'))
                                doc_title = (_title[:200] if _title and len(_title) > 200 else _title)
                                chapter = to_text(ch.get('chapter'))
                                section = to_text(ch.get('section'))
                                article = to_text(ch.get('article'))
                                article_heading = to_text(ch.get('article_heading'))
                                clause = to_text(ch.get('clause'))
                                point = to_text(ch.get('point'))
                                chunk_index = to_int(ch.get('chunk_index'))
                                # NÃ©n content nháº¹: bá» khoáº£ng tráº¯ng thá»«a
                                _content = to_text(ch.get('content'))
                                content = ' '.join(_content.split()) if _content else None
                                word_count = ch.get('word_count')
                                if word_count is None and content is not None:
                                    word_count = len(content.split())
                                word_count = to_int(word_count)
                                chunk_type = to_text(ch.get('chunk_type'))

                                rows.append((
                                    chunk_id,
                                    doc_file,
                                    doc_title,
                                    chapter,
                                    section,
                                    article,
                                    article_heading,
                                    clause,
                                    point,
                                    chunk_index,
                                    content,
                                    word_count,
                                    chunk_type,
                                ))

                            if rows:
                                cur.executemany(
                                    'INSERT INTO chunks (chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)',
                                    rows
                                )
                                total_chunks += len(rows)
                                # Commit Ä‘á»‹nh ká»³ Ä‘á»ƒ an toÃ n
                                if total_chunks % 5000 == 0:
                                    conn.commit()

                            # Hiá»ƒn thá»‹ status má»—i 10 files
                            if total_processed % 10 == 0:
                                elapsed_time = time.time() - start_time
                                files_per_second = total_processed / elapsed_time if elapsed_time > 0 else 0
                                pbar.set_postfix({
                                    'processed': f"{total_processed}/{len(xml_files)}",
                                    'speed': f"{files_per_second:.1f} files/s",
                                    'chunks': f"{total_chunks:,}"
                                })

                        except Exception as e:
                            print(f"âŒ Lá»—i xá»­ lÃ½ {xml_file.name}: {e}")
                            total_processed += 1
                            pbar.update(1)

                    # Commit má»—i batch
                    conn.commit()
                    print(f"ğŸ’¾ ÄÃ£ commit sau {total_processed} files, tá»•ng chunks: {total_chunks:,}")

        # HoÃ n táº¥t SQLite
        conn.commit()
        conn.close()

        # Táº¡o Parquet tá»« SQLite (náº¿u cÃ³ pyarrow vÃ  khÃ´ng skip)
        parquet_ok = False
        if not skip_parquet:
            try:
                import pandas as pd  # type: ignore
                try:
                    import pyarrow  # noqa: F401
                    has_pa = True
                except Exception:
                    has_pa = False
                if has_pa:
                    conn = sqlite3.connect(str(db_path))
                    df = pd.read_sql_query(
                        'SELECT chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type FROM chunks ORDER BY chunk_id',
                        conn
                    )
                    df.to_parquet(parquet_path, engine='pyarrow', index=False)
                    conn.close()
                    parquet_ok = True
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ xuáº¥t Parquet: {e}")

        elapsed_time = time.time() - start_time
        files_per_second = total_processed / elapsed_time if elapsed_time > 0 else 0

        print("\nâœ… HOÃ€N THÃ€NH Xá»¬ LÃ á»”N Äá»ŠNH!")
        print(f"ğŸ“Š Káº¾T QUáº¢ Tá»”NG QUAN:")
        print(f"   - Tá»•ng thá»i gian: {elapsed_time:.1f} giÃ¢y")
        print(f"   - Tá»•ng chunks: {total_chunks:,}")

        print(f"ğŸ’¾ SQLite: {db_path}")
        if parquet_ok:
            print(f"ğŸ’¾ Parquet: {parquet_path}")

def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ VNLEGAL TEXT PROCESSOR - VERSION á»”N Äá»ŠNH")
    print("=" * 60)

    parser = argparse.ArgumentParser(description='Stable VNLegalText Preprocessor')
    parser.add_argument('--batch-size', type=int, default=100, help='Sá»‘ file xá»­ lÃ½ má»—i batch (máº·c Ä‘á»‹nh: 100)')
    parser.add_argument('--skip-parquet', action='store_true', help='Bá» qua bÆ°á»›c xuáº¥t Parquet Ä‘á»ƒ tÄƒng tá»‘c')
    parser.add_argument('--fast', action='store_true', help='Cháº¿ Ä‘á»™ nhanh: bá» cleaned_for_ui vÃ  thá»‘ng kÃª')
    parser.add_argument('--input', type=str, default=None, help='ÄÆ°á»ng dáº«n input: thÆ° má»¥c XML hoáº·c file corpus.jsonl / parquet (máº·c Ä‘á»‹nh: data/raw/VNLegalText)')
    args = parser.parse_args()

    try:
        processor = StableVNLegalTextProcessor(fast=args.fast)
        print("âœ… Khá»Ÿi táº¡o processor thÃ nh cÃ´ng")
        print("âœ… PyVi: ÄÃ£ sáºµn sÃ ng")
        print(f"âœ… Stopwords: {len(processor.stopwords)} tá»«")
    except ImportError as e:
        print(f"âŒ Lá»–I KHá»I Táº O: {e}")
        return
    except ValueError as e:
        print(f"âŒ Lá»–I KHá»I Táº O: {e}")
        return

    # ÄÆ°á»ng dáº«n
    root = get_project_root()
    # Náº¿u user cung cáº¥p --input, dÃ¹ng Ä‘Æ°á»ng dáº«n Ä‘Ã³ (cÃ³ thá»ƒ lÃ  file hoáº·c thÆ° má»¥c)
    if args.input:
        input_path = Path(args.input)
        if not input_path.is_absolute():
            input_path = root / args.input
    else:
        input_path = root / "data" / "raw" / "VNLegalText"

    # Giá»¯ biáº¿n output_path cho tÆ°Æ¡ng thÃ­ch, nhÆ°ng khÃ´ng xuáº¥t JSON ná»¯a
    output_path = root / "data" / "processed" / "vnlegaltext_stable.json"

    # Cáº¥u hÃ¬nh batch size
    batch_size = int(args.batch_size)

    print(f"ğŸ“‚ Input: {input_path}")
    print(f"ğŸ“‚ Output (DB/Parquet sáº½ náº±m cÃ¹ng thÆ° má»¥c): {output_path.parent}")
    print(f"ğŸ”¢ Batch size: {batch_size}")
    if args.fast:
        print("âš¡ Fast mode: Bá» cleaned_for_ui vÃ  thá»‘ng kÃª")
    if args.skip_parquet:
        print("ğŸ§ª Skip Parquet: Chá»‰ xuáº¥t SQLite")
    print("ğŸ“‹ Xá»­ lÃ½: PyVi + Stopwords removal (Báº®T BUá»˜C)")
    print()

    processor.process_all_files_stable(input_path, output_path, batch_size, skip_parquet=args.skip_parquet)

if __name__ == "__main__":
    main()
