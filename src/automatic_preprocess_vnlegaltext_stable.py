#!/usr/bin/env python3
"""
Version á»•n Ä‘á»‹nh cá»§a automatic_preprocess_vnlegaltext.py
- Checkpoint saving: LÆ°u tiáº¿n trÃ¬nh má»—i 100 files
- Memory management: Xá»­ lÃ½ theo batch nhá»
- Error recovery: Skip file lá»—i, tiáº¿p tá»¥c xá»­ lÃ½
- Resume capability: CÃ³ thá»ƒ tiáº¿p tá»¥c tá»« checkpoint
"""

import json
import re
import time
from pathlib import Path
try:
    # Khi cháº¡y dáº¡ng module: python -m src.automatic_preprocess_vnlegaltext_stable
    from src.utils.paths import get_project_root
except Exception:
    # Khi cháº¡y trá»±c tiáº¿p: python src/automatic_preprocess_vnlegaltext_stable.py
    from utils.paths import get_project_root  # type: ignore
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import sqlite3
import argparse

# Sinh smart chunks trá»±c tiáº¿p khÃ´ng qua JSON
try:
    from src.export_chunks_storage import build_chunks_for_document
except Exception:
    from export_chunks_storage import build_chunks_for_document  # type: ignore

# Import PyVi
try:
    from pyvi import ViTokenizer
    HAS_PYVI = True
except ImportError:
    HAS_PYVI = False

class StableVNLegalTextProcessor:
    """Version á»•n Ä‘á»‹nh vá»›i checkpoint vÃ  error recovery"""

    def __init__(self, fast: bool = False):
        """Khá»Ÿi táº¡o processor"""
        self.fast = fast
        self.patterns = {
            'article': re.compile(r'^\s*Äiá»u\s+(\d+)\s*\.\s*(.*)$', re.MULTILINE),
            'clause': re.compile(r'^\s*(\d+)\s*\.(?:\s+|\s*)(.*)$', re.MULTILINE),
            # Äiá»ƒm: cháº¥p nháº­n a) hoáº·c a ) hoáº·c (a), bao gá»“m chá»¯ cÃ³ dáº¥u (Ä‘, Ã¢, Ãª, Ã´, Äƒ, Æ¡, Æ° ...)
            'point': re.compile(r'^\s*(?:\(|\s*)?([a-zÃ Ã¡áº¡áº£Ã£Äƒáº±áº¯áº·áº³áºµÃ¢áº§áº¥áº­áº©áº«Ä‘Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹])\s*\)\s+(.*)$', re.MULTILINE | re.IGNORECASE),
            'chapter': re.compile(r'^\s*CHÆ¯Æ NG\s+([IVXLCDM]+|[0-9]+)\b\s*(.*)$', re.MULTILINE | re.IGNORECASE),
            'section': re.compile(r'^\s*Má»¥c\s+(\d+)\b\s*(.*)$', re.MULTILINE | re.IGNORECASE)
        }

        # Mapping sá»‘ La MÃ£
        self.roman_numerals = {
            'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
            'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,
            'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15
        }

        # Táº£i stopwords - Báº®T BUá»˜C
        self.stopwords = self._load_stopwords()

        # Kiá»ƒm tra cÃ¡c dependencies báº¯t buá»™c
        if not HAS_PYVI:
            raise ImportError("âŒ PyVi is required but not available. Please install pyvi: pip install pyvi")

        if not self.stopwords:
            raise ValueError("âŒ Stopwords file is required but not available or empty")

    def _load_stopwords(self) -> set:
        """Táº£i stopwords tá»« file"""
        try:
            stopwords_file = Path(__file__).parent.parent / "data" / "vietnamese-stopwords-dash.txt"
            if stopwords_file.exists():
                with open(stopwords_file, 'r', encoding='utf-8') as f:
                    stopwords = set(line.strip().lower() for line in f if line.strip())
                print(f"âœ… ÄÃ£ táº£i {len(stopwords)} stopwords")
                return stopwords
            else:
                print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y file stopwords")
                return set()
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi táº£i stopwords: {e}")
            return set()

    def _load_checkpoint(self, checkpoint_file: Path) -> Dict[str, Any]:
        """Táº£i checkpoint Ä‘á»ƒ resume"""
        if checkpoint_file.exists():
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ táº£i checkpoint: {e}")
        return {'processed_files': [], 'last_index': 0, 'start_time': time.time()}

    def _save_checkpoint(self, checkpoint_file: Path, processed_data: List[Dict], last_index: int, start_time: float):
        """LÆ°u checkpoint"""
        try:
            checkpoint = {
                'processed_files': processed_data,
                'last_index': last_index,
                'start_time': start_time,
                'timestamp': time.time()
            }
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  Lá»—i khi lÆ°u checkpoint: {e}")

    def clean_text_with_vietnamese_support(self, text: str, remove_stopwords: bool = False, keep_underscore: bool = True) -> str:
        """LÃ m sáº¡ch vÄƒn báº£n vá»›i PyVi.

        - remove_stopwords: True Ä‘á»ƒ loáº¡i bá» stopwords (dÃ¹ng cho UI), False cho index
        - keep_underscore: True Ä‘á»ƒ giá»¯ "_" cá»§a ViTokenizer (dÃ¹ng cho index)
        """
        if not text:
            return ""

        try:
            # XÃ³a cÃ¡c tag XML nhÆ°ng thay báº±ng khoáº£ng tráº¯ng Ä‘á»ƒ trÃ¡nh dÃ­nh tá»«
            text = re.sub(r'<[^>]+>', ' ', text)

            # Chuáº©n hÃ³a khoáº£ng tráº¯ng nhÆ°ng giá»¯ láº¡i cáº¥u trÃºc dÃ²ng
            text = re.sub(r'[ \t]+', ' ', text)
            # ChÃ¨n khoáº£ng tráº¯ng trÆ°á»›c khi tokenize Ä‘á»ƒ trÃ¡nh dÃ­nh tá»« cáº¡nh dáº¥u cÃ¢u
            text = re.sub(r'([,;:\.!?)(\[\]"\'])', r' \1 ', text)
            text = re.sub(r'\s+', ' ', text)

            # Sá»­ dá»¥ng pyvi Ä‘á»ƒ tÃ¡ch tá»« tá»± Ä‘á»™ng - Báº®T BUá»˜C
            text = ViTokenizer.tokenize(text)

            # Loáº¡i bá» stopwords náº¿u Ä‘Æ°á»£c yÃªu cáº§u
            if remove_stopwords:
                words = text.split()
                words = [word for word in words if word.lower() not in self.stopwords]
                text = ' '.join(words)

            # Xá»­ lÃ½ dáº¥u gáº¡ch dÆ°á»›i tá»« ViTokenizer
            if not keep_underscore:
                text = text.replace('_', ' ')

            # LÃ m sáº¡ch nháº¹ nhÃ ng - GIá»® Láº I Dáº¤U TIáº¾NG VIá»†T VÃ€ KÃ HIá»†U PHá»” BIáº¾N
            pattern = r"[^a-zA-Z0-9\sÃ€-á»¹Ã -á»¹\.,;:!\?()\[\]\"'\-\n_â€“â€”â€¦/Â§â€œâ€â€˜â€™]"
            text = re.sub(pattern, '', text)

            return text.strip()

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi xá»­ lÃ½ text: {e}")
            return text[:500] if text else ""  # Return truncated text as fallback

    def _normalize_underscore_separated_words(self, text: str) -> str:
        """Deprecated: KhÃ´ng sá»­ dá»¥ng trong pipeline chÃ­nh."""
        return text

    def parse_legal_structure(self, content: str, document_title: str = "") -> List[Dict[str, Any]]:
        """Parse cáº¥u trÃºc phÃ¡p lÃ½ (CHÆ¯Æ NG â†’ Má»¤C â†’ Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm)."""
        sections: List[Dict[str, Any]] = []

        try:
            lines = content.split('\n')

            current_chapter: Optional[Dict[str, Any]] = None
            current_section: Optional[Dict[str, Any]] = None
            current_article: Optional[Dict[str, Any]] = None
            current_clause: Optional[Dict[str, Any]] = None

            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue

                # CHÆ¯Æ NG
                chapter_match = self.patterns['chapter'].search(line)
                if chapter_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)
                        current_article = None
                    if current_section:
                        sections.append(current_section)
                        current_section = None
                    if current_chapter:
                        sections.append(current_chapter)

                    chap_num = chapter_match.group(1)
                    chap_title = chapter_match.group(2).strip() if chapter_match.lastindex and chapter_match.group(2) else ""
                    current_chapter = {
                        'title': f"CHÆ¯Æ NG {chap_num}",
                        'heading': chap_title,
                        'content': "",
                        'section_type': 'chapter',
                        'section_number': chap_num,
                        'level': 0
                    }
                    continue

                # Má»¤C
                section_match = self.patterns['section'].search(line)
                if section_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)
                        current_article = None
                    if current_section:
                        sections.append(current_section)

                    sec_num = section_match.group(1)
                    sec_title = section_match.group(2).strip() if section_match.lastindex and section_match.group(2) else ""
                    current_section = {
                        'title': f"Má»¥c {sec_num}",
                        'heading': sec_title,
                        'content': "",
                        'section_type': 'section',
                        'section_number': sec_num,
                        'parent_section': current_chapter['title'] if current_chapter else None,
                        'level': 0.5
                    }
                    continue

                # Äiá»u
                article_match = self.patterns['article'].search(line)
                if article_match:
                    if current_clause:
                        sections.append(current_clause)
                        current_clause = None
                    if current_article:
                        sections.append(current_article)

                    article_num = article_match.group(1)
                    article_heading = article_match.group(2).strip() if article_match.lastindex and article_match.group(2) else ""
                    current_article = {
                        'title': f"Äiá»u {article_num}",
                        'heading': article_heading,
                        'content': "",
                        'section_type': 'article',
                        'section_number': article_num,
                        'parent_section': current_section['title'] if current_section else (current_chapter['title'] if current_chapter else None),
                        'level': 1
                    }
                    continue

                # Khoáº£n
                clause_match = self.patterns['clause'].search(line)
                if clause_match and current_article:
                    if current_clause:
                        sections.append(current_clause)

                    clause_num = clause_match.group(1)
                    clause_content = clause_match.group(2).strip()
                    current_clause = {
                        'title': f"Khoáº£n {clause_num}",
                        'content': clause_content,
                        'section_type': 'clause',
                        'section_number': clause_num,
                        'parent_section': current_article['title'],
                        'level': 2
                    }
                    continue

                # Äiá»ƒm
                point_match = self.patterns['point'].search(line)
                if point_match and current_clause:
                    point_label = point_match.group(1)
                    point_content = point_match.group(2).strip()
                    sections.append({
                        'title': f"Äiá»ƒm {point_label})",
                        'content': point_content,
                        'section_type': 'point',
                        'section_number': point_label,
                        'parent_section': current_clause['title'],
                        'level': 3
                    })
                    continue

                # Ná»™i dung
                if current_clause:
                    current_clause['content'] = (current_clause['content'] + ' ' + line).strip() if current_clause.get('content') else line
                elif current_article:
                    current_article['content'] = (current_article['content'] + ' ' + line).strip() if current_article.get('content') else line
                elif current_section:
                    current_section['content'] = (current_section['content'] + ' ' + line).strip() if current_section.get('content') else line
                elif current_chapter:
                    current_chapter['content'] = (current_chapter['content'] + ' ' + line).strip() if current_chapter.get('content') else line

            # Flush cuá»‘i cÃ¹ng
            if current_clause:
                sections.append(current_clause)
            if current_article:
                sections.append(current_article)
            if current_section:
                sections.append(current_section)
            if current_chapter:
                sections.append(current_chapter)

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi parse structure: {e}")

        return sections

    def extract_title_and_metadata(self, content: str, raw_text: Optional[str] = None) -> Dict[str, Any]:
        """TrÃ­ch xuáº¥t tiÃªu Ä‘á» vÃ  metadata cÆ¡ báº£n (heuristic)."""
        try:
            lines = content.split('\n')[:10]  # Chá»‰ xem pháº§n Ä‘áº§u

            metadata = {
                'title': 'Unknown',
                'document_type': 'legal_document',
                'language': 'vietnamese'
            }

            # Æ¯u tiÃªn dÃ²ng Ä‘áº§u khÃ´ng rá»—ng lÃ m tiÃªu Ä‘á» náº¿u chá»©a tá»« khÃ³a phÃ¡p lÃ½
            for line in lines:
                line = line.strip()
                if not line or line.startswith('<'):
                    continue
                low = line.lower()
                if any(k in low for k in ['luáº­t', 'nghá»‹ Ä‘á»‹nh', 'thÃ´ng tÆ°', 'quyáº¿t Ä‘á»‹nh', 'hiáº¿n phÃ¡p']):
                    metadata['title'] = line
                    break

            # Heuristic tá»« raw_text cho cÃ¡c trÆ°á»ng phá»• biáº¿n + ngÃ y thÃ¡ng (hiá»‡u lá»±c / thÃ´ng qua)
            if raw_text:
                raw_norm = raw_text.replace('_', ' ')

                # VÃ¹ng Ä‘áº§u vÄƒn báº£n: sá»‘, cÆ¡ quan, tÃ¬nh tráº¡ng
                head = '\n'.join(raw_norm.splitlines()[:200])
                patterns = {
                    'reference_number': r'\bSá»‘[:\s]+([^\n]+)',
                    'issuing_body': r'\bCÆ¡\s*quan\s*ban\s*hÃ nh[:\s]+([^\n]+)',
                    'status': r'\bTÃ¬nh\s*tráº¡ng[:\s]+([^\n]+)'
                }
                for key, pat in patterns.items():
                    m = re.search(pat, head, flags=re.IGNORECASE)
                    if m:
                        metadata[key] = m.group(1).strip()

                # TÃ¬m hiá»‡u lá»±c thi hÃ nh (Æ°u tiÃªn Ä‘iá»u Hiá»‡u_lá»±c thi_hÃ nh)
                eff_match = re.search(r'hiá»‡u\s*lá»±c[^\n]*?tá»«\s+ngÃ y\s+(\d{1,2})\s+thÃ¡ng\s+(\d{1,2})\s+nÄƒm\s+(\d{4})', raw_norm, flags=re.IGNORECASE)
                if not eff_match:
                    eff_match = re.search(r'(\d{1,2})[\/-](\d{1,2})[\/-](\d{4}).{0,40}hiá»‡u\s*lá»±c', raw_norm, flags=re.IGNORECASE)
                if eff_match:
                    try:
                        d, mth, y = int(eff_match.group(1)), int(eff_match.group(2)), int(eff_match.group(3))
                        metadata['effective_date'] = f"{y:04d}-{mth:02d}-{d:02d}"
                        metadata['effective_year'] = y
                    except Exception:
                        pass

                # TÃ¬m ngÃ y thÃ´ng qua á»Ÿ pháº§n cuá»‘i
                tail = '\n'.join(raw_norm.splitlines()[-200:])
                prom_match = re.search(r'thÃ´ng\s*qua\s+ngÃ y\s+(\d{1,2})\s+thÃ¡ng\s+(\d{1,2})\s+nÄƒm\s+(\d{4})', tail, flags=re.IGNORECASE)
                if prom_match:
                    try:
                        d, mth, y = int(prom_match.group(1)), int(prom_match.group(2)), int(prom_match.group(3))
                        metadata['promulgation_date'] = f"{y:04d}-{mth:02d}-{d:02d}"
                        metadata['promulgation_year'] = y
                    except Exception:
                        pass

                # TrÃ­ch citations tá»« tag <L|HP>
                cites = []
                for m in re.finditer(r'<(L|HP)([^>]*)>(.*?)</\1>', raw_text, flags=re.IGNORECASE | re.DOTALL):
                    tag = m.group(1)
                    attrs = m.group(2) or ''
                    txt = re.sub(r'<[^>]+>', ' ', m.group(3)).strip()
                    rel_m = re.search(r'rel\s*=\s*"([^"]+)"', attrs)
                    cites.append({'type': tag.upper(), 'rel': rel_m.group(1) if rel_m else None, 'text': txt})
                if cites:
                    metadata['citations'] = cites

            return metadata

        except Exception as e:
            print(f"âš ï¸  Lá»—i khi extract metadata: {e}")
            return {'title': 'Error', 'document_type': 'unknown', 'language': 'vietnamese'}

    def process_single_file(self, file_path: Path) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t file XML (version á»•n Ä‘á»‹nh)"""
        try:
            # Äá»c file vá»›i error handling
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    raw_content = f.read()
            except UnicodeDecodeError:
                # Thá»­ encoding khÃ¡c
                with open(file_path, 'r', encoding='utf-8-sig') as f:
                    raw_content = f.read()
            except Exception as e:
                return {
                    'file_name': file_path.name,
                    'file_path': str(file_path),
                    'error': f"Cannot read file: {e}",
                    'error_type': 'file_read_error'
                }

            # Giá»›i háº¡n kÃ­ch thÆ°á»›c file (trÃ¡nh memory issue)
            if len(raw_content) > 10 * 1024 * 1024:  # 10MB
                return {
                    'file_name': file_path.name,
                    'file_path': str(file_path),
                    'error': "File too large (>10MB)",
                    'error_type': 'file_too_large'
                }

            # LÃ m sáº¡ch vÄƒn báº£n (hai báº£n: cho index vÃ  cho UI)
            cleaned_for_index = self.clean_text_with_vietnamese_support(
                raw_content,
                remove_stopwords=False,
                keep_underscore=True
            )
            cleaned_for_ui = None
            if not self.fast:
                cleaned_for_ui = self.clean_text_with_vietnamese_support(
                    raw_content,
                    remove_stopwords=True,
                    keep_underscore=False
                )

            # TrÃ­ch xuáº¥t metadata
            metadata = self.extract_title_and_metadata(cleaned_for_index, raw_text=raw_content)

            # Parse cáº¥u trÃºc phÃ¡p lÃ½
            document_title = metadata.get('title', file_path.name.replace('.xml', ''))
            sections = self.parse_legal_structure(cleaned_for_index, document_title)

            # Táº¡o káº¿t quáº£
            result = {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'metadata': metadata,
                'cleaned_content': cleaned_for_index,
                'cleaned_content_index': cleaned_for_index,
                'cleaned_content_ui': cleaned_for_ui if cleaned_for_ui is not None else '',
                'sections': sections,
                'stats': {} if self.fast else {
                    'total_sections': len(sections),
                    'articles_count': len([s for s in sections if s['section_type'] == 'article']),
                    'clauses_count': len([s for s in sections if s['section_type'] == 'clause']),
                    'points_count': len([s for s in sections if s['section_type'] == 'point']),
                    'total_words': len(cleaned_for_index.split())
                },
                'processing_info': {
                    'used_pyvi': True,
                    'stopwords_removed_for_index': False,
                    'stopwords_removed_for_ui': (cleaned_for_ui is not None),
                    'processing_time': time.time(),
                    'file_size': len(raw_content)
                }
            }

            return result

        except Exception as e:
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'error': str(e),
                'error_type': type(e).__name__
            }

    def process_all_files_stable(self, input_dir: Path, output_path: Path, batch_size: int = 50, skip_parquet: bool = False):
        """Xá»­ lÃ½ táº¥t cáº£ files vÃ  xuáº¥t tháº³ng smart chunks vÃ o SQLite/Parquet, khÃ´ng táº¡o JSON tá»•ng."""
        # Táº¡o thÆ° má»¥c output
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # ÄÃ­ch xuáº¥t tá»‘i Æ°u
        processed_dir = output_path.parent
        db_path = processed_dir / 'smart_chunks_stable.db'
        parquet_path = processed_dir / 'smart_chunks_stable.parquet'

        # XÃ³a output cÅ© Ä‘á»ƒ luÃ´n táº¡o DB/Parquet má»›i (trÃ¡nh phÃ¬nh file khi reprocess)
        try:
            if db_path.exists():
                db_path.unlink()
            if parquet_path.exists():
                parquet_path.unlink()
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ xÃ³a output cÅ©: {e}. Sáº½ ghi Ä‘Ã¨ báº£ng thay tháº¿.")

        # Khá»Ÿi táº¡o SQLite (táº¡o má»›i báº£ng)
        conn = sqlite3.connect(str(db_path))
        cur = conn.cursor()
        # PRAGMA tá»‘i Æ°u ghi nhanh (Ä‘Ã¡nh Ä‘á»•i Ä‘á»™ bá»n trong phiÃªn xá»­ lÃ½ batch)
        try:
            cur.execute("PRAGMA journal_mode=OFF;")
            cur.execute("PRAGMA synchronous=OFF;")
            cur.execute("PRAGMA temp_store=MEMORY;")
            cur.execute("PRAGMA cache_size=-200000;")
        except Exception:
            pass
        cur.execute('DROP TABLE IF EXISTS chunks')
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS chunks (
                chunk_id      INTEGER PRIMARY KEY,
                doc_file      TEXT,
                doc_title     TEXT,
                chapter       TEXT,
                section       TEXT,
                article       TEXT,
                article_heading TEXT,
                clause        TEXT,
                point         TEXT,
                chunk_index   INTEGER,
                content       TEXT,
                word_count    INTEGER,
                chunk_type    TEXT,
                effective_date TEXT,
                effective_year INTEGER,
                promulgation_date TEXT,
                promulgation_year INTEGER,
                citations     TEXT
            )
            """
        )

        def to_int(value):
            if value is None:
                return None
            try:
                return int(value)
            except Exception:
                return None

        def to_text(value):
            if value is None:
                return None
            try:
                return str(value)
            except Exception:
                return None

        total_chunks = 0

        # Láº¥y danh sÃ¡ch file XML
        xml_files = list(input_dir.glob("*.xml"))

        if not xml_files:
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file XML nÃ o trong {input_dir}")
            # ÄÃ³ng káº¿t ná»‘i náº¿u Ä‘Ã£ má»Ÿ
            conn.close()
            return

        print(f"ğŸ“ TÃ¬m tháº¥y {len(xml_files)} file XML")
        print(f"ğŸ¤– Sá»­ dá»¥ng xá»­ lÃ½ á»•n Ä‘á»‹nh: PyVi {'CÃ³' if HAS_PYVI else 'KhÃ´ng'}")
        print(f"ğŸ“Š Batch size: {batch_size} files")

        # KhÃ´ng sá»­ dá»¥ng JSON checkpoint lá»›n; theo dÃµi tá»‘i thiá»ƒu báº±ng biáº¿n Ä‘áº¿m
        start_time = time.time()
        total_processed = 0

        remaining_files = xml_files
        print(f"ğŸ”„ Báº¯t Ä‘áº§u xá»­ lÃ½ {len(xml_files)} files")

        if not remaining_files:
            print("âœ… Táº¥t cáº£ files Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½!")
            return

        # Xá»­ lÃ½ theo batch
        # Báº¯t Ä‘áº§u Ä‘áº¿m tá»« 0
        # total_processed Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o á»Ÿ trÃªn

        with tqdm(total=len(remaining_files), desc="Processing XML files", unit="file") as pbar:
            for i in range(0, len(remaining_files), batch_size):
                batch = remaining_files[i:i + batch_size]

                for xml_file in batch:
                    try:
                        result = self.process_single_file(xml_file)

                        # Cáº­p nháº­t progress
                        total_processed += 1
                        pbar.update(1)

                        # Bá» qua file lá»—i
                        if 'error' in result:
                            continue

                        # XÃ¢y smart chunks cho tÃ i liá»‡u nÃ y
                        doc_chunks = build_chunks_for_document(result, start_chunk_id=total_chunks)

                        # Láº¥y ngÃ y thÃ¡ng/citations tá»« metadata (náº¿u cÃ³)
                        eff_date = result.get('metadata', {}).get('effective_date')
                        eff_year = result.get('metadata', {}).get('effective_year')
                        prom_date = result.get('metadata', {}).get('promulgation_date')
                        prom_year = result.get('metadata', {}).get('promulgation_year')
                        citations = result.get('metadata', {}).get('citations')
                        citations_json = json.dumps(citations, ensure_ascii=False) if citations else None

                        # Chuáº©n hÃ³a vÃ  chÃ¨n vÃ o SQLite
                        rows = []
                        for ch in doc_chunks:
                            chunk_id = to_int(ch.get('chunk_id'))
                            doc_file = to_text(ch.get('doc_file'))
                            # RÃºt gá»n doc_title Ä‘á»ƒ giáº£m size: cáº¯t tá»‘i Ä‘a 200 kÃ½ tá»±
                            _title = to_text(ch.get('doc_title'))
                            doc_title = (_title[:200] if _title and len(_title) > 200 else _title)
                            chapter = to_text(ch.get('chapter'))
                            section = to_text(ch.get('section'))
                            article = to_text(ch.get('article'))
                            article_heading = to_text(ch.get('article_heading'))
                            clause = to_text(ch.get('clause'))
                            point = to_text(ch.get('point'))
                            chunk_index = to_int(ch.get('chunk_index'))
                            # NÃ©n content nháº¹: bá» khoáº£ng tráº¯ng thá»«a
                            _content = to_text(ch.get('content'))
                            content = ' '.join(_content.split()) if _content else None
                            word_count = ch.get('word_count')
                            if word_count is None and content is not None:
                                word_count = len(content.split())
                            word_count = to_int(word_count)
                            chunk_type = to_text(ch.get('chunk_type'))

                            rows.append((
                                chunk_id,
                                doc_file,
                                doc_title,
                                chapter,
                                section,
                                article,
                                article_heading,
                                clause,
                                point,
                                chunk_index,
                                content,
                                word_count,
                                chunk_type,
                                eff_date,
                                to_int(eff_year),
                                prom_date,
                                to_int(prom_year),
                                citations_json,
                            ))

                        if rows:
                            cur.executemany(
                                'INSERT INTO chunks (chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type, effective_date, effective_year, promulgation_date, promulgation_year, citations) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)',
                                rows
                            )
                            total_chunks += len(rows)
                            # Commit Ä‘á»‹nh ká»³ Ä‘á»ƒ an toÃ n
                            if total_chunks % 5000 == 0:
                                conn.commit()

                        # Hiá»ƒn thá»‹ status má»—i 10 files
                        if total_processed % 10 == 0:
                            elapsed_time = time.time() - start_time
                            files_per_second = total_processed / elapsed_time if elapsed_time > 0 else 0
                            pbar.set_postfix({
                                'processed': f"{total_processed}/{len(xml_files)}",
                                'speed': f"{files_per_second:.1f} files/s",
                                'chunks': f"{total_chunks:,}"
                            })

                    except Exception as e:
                        print(f"âŒ Lá»—i xá»­ lÃ½ {xml_file.name}: {e}")
                        total_processed += 1
                        pbar.update(1)

                # Commit má»—i batch
                conn.commit()
                print(f"ğŸ’¾ ÄÃ£ commit sau {total_processed} files, tá»•ng chunks: {total_chunks:,}")

        # HoÃ n táº¥t SQLite
        conn.commit()
        conn.close()

        # Táº¡o Parquet tá»« SQLite (náº¿u cÃ³ pyarrow vÃ  khÃ´ng skip)
        parquet_ok = False
        if not skip_parquet:
            try:
                import pandas as pd  # type: ignore
                try:
                    import pyarrow  # noqa: F401
                    has_pa = True
                except Exception:
                    has_pa = False
                if has_pa:
                    conn = sqlite3.connect(str(db_path))
                    df = pd.read_sql_query(
                        'SELECT chunk_id, doc_file, doc_title, chapter, section, article, article_heading, clause, point, chunk_index, content, word_count, chunk_type, effective_date, effective_year, promulgation_date, promulgation_year, citations FROM chunks ORDER BY chunk_id',
                        conn
                    )
                    df.to_parquet(parquet_path, engine='pyarrow', index=False)
                    conn.close()
                    parquet_ok = True
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ xuáº¥t Parquet: {e}")

        elapsed_time = time.time() - start_time
        files_per_second = total_processed / elapsed_time if elapsed_time > 0 else 0

        print("\nâœ… HOÃ€N THÃ€NH Xá»¬ LÃ á»”N Äá»ŠNH!")
        print(f"ğŸ“Š Káº¾T QUáº¢ Tá»”NG QUAN:")
        print(f"   - Tá»•ng thá»i gian: {elapsed_time:.1f} giÃ¢y")
        print(f"   - Tá»•ng chunks: {total_chunks:,}")

        print(f"ğŸ’¾ SQLite: {db_path}")
        if parquet_ok:
            print(f"ğŸ’¾ Parquet: {parquet_path}")

def main():
    """HÃ m chÃ­nh"""
    print("ğŸš€ VNLEGAL TEXT PROCESSOR - VERSION á»”N Äá»ŠNH")
    print("=" * 60)

    parser = argparse.ArgumentParser(description='Stable VNLegalText Preprocessor')
    parser.add_argument('--batch-size', type=int, default=100, help='Sá»‘ file xá»­ lÃ½ má»—i batch (máº·c Ä‘á»‹nh: 100)')
    parser.add_argument('--skip-parquet', action='store_true', help='Bá» qua bÆ°á»›c xuáº¥t Parquet Ä‘á»ƒ tÄƒng tá»‘c')
    parser.add_argument('--fast', action='store_true', help='Cháº¿ Ä‘á»™ nhanh: bá» cleaned_for_ui vÃ  thá»‘ng kÃª')
    args = parser.parse_args()

    try:
        processor = StableVNLegalTextProcessor(fast=args.fast)
        print("âœ… Khá»Ÿi táº¡o processor thÃ nh cÃ´ng")
        print("âœ… PyVi: ÄÃ£ sáºµn sÃ ng")
        print(f"âœ… Stopwords: {len(processor.stopwords)} tá»«")
    except ImportError as e:
        print(f"âŒ Lá»–I KHá»I Táº O: {e}")
        return
    except ValueError as e:
        print(f"âŒ Lá»–I KHá»I Táº O: {e}")
        return

    # ÄÆ°á»ng dáº«n
    root = get_project_root()
    vnlegal_path = root / "data" / "raw" / "VNLegalText"
    # Giá»¯ biáº¿n output_path cho tÆ°Æ¡ng thÃ­ch, nhÆ°ng khÃ´ng xuáº¥t JSON ná»¯a
    output_path = root / "data" / "processed" / "vnlegaltext_stable.json"

    # Cáº¥u hÃ¬nh batch size
    batch_size = int(args.batch_size)

    print(f"ğŸ“‚ Input: {vnlegal_path}")
    print(f"ğŸ“‚ Output (DB/Parquet sáº½ náº±m cÃ¹ng thÆ° má»¥c): {output_path.parent}")
    print(f"ğŸ”¢ Batch size: {batch_size}")
    if args.fast:
        print("âš¡ Fast mode: Bá» cleaned_for_ui vÃ  thá»‘ng kÃª")
    if args.skip_parquet:
        print("ğŸ§ª Skip Parquet: Chá»‰ xuáº¥t SQLite")
    print("ğŸ“‹ Xá»­ lÃ½: PyVi + Stopwords removal (Báº®T BUá»˜C)")
    print()

    processor.process_all_files_stable(vnlegal_path, output_path, batch_size, skip_parquet=args.skip_parquet)

if __name__ == "__main__":
    main()
