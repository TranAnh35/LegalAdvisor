#!/usr/bin/env python3
"""
Chu·∫©n h√≥a d·ªØ li·ªáu cho Retriever (VNLAWQC, VNSynLawQC):

- Tr√≠ch (query, positive_text) t·ª´ d·ªØ li·ªáu th√¥, √°nh x·∫° positive_text ‚Üí chunk_id b·∫±ng FAISS hi·ªán c√≥
- Sinh hard negatives b·∫±ng c√°ch truy h·ªìi topK r·ªìi lo·∫°i b·ªè positive
- Ghi ra JSONL: {query, positive_id, hard_negatives, positive_score, source}

Ch·∫°y v√≠ d·ª• (PowerShell, Windows):

  conda activate LegalAdvisor
  python -m src.datasets.retrieval_prepare \
    --input data/raw/VNLAWQC.jsonl data/raw/VNSynLawQC.jsonl \
    --output data/processed/retrieval_train.jsonl --hard-negatives 15 --dense-top-k 64

N·∫øu kh√¥ng truy·ªÅn --input, script s·∫Ω c·ªë g·∫Øng d√≤ c√°c file ph·ªï bi·∫øn trong data/raw/.
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

# B·∫£o ƒë·∫£m import ƒë∆∞·ª£c service/paths
try:
    # Khi ch·∫°y d∆∞·ªõi d·∫°ng module: python -m src.datasets.retrieval_prepare
    from src.retrieval.service import RetrievalService
    from src.utils.paths import get_project_root, get_processed_data_dir
except Exception:
    # Khi ch·∫°y tr·ª±c ti·∫øp: python src/datasets/retrieval_prepare.py
    THIS_FILE = Path(__file__).resolve()
    SRC_DIR = THIS_FILE.parent.parent
    sys.path.append(str(SRC_DIR))
    from retrieval.service import RetrievalService  # type: ignore
    from utils.paths import get_project_root, get_processed_data_dir  # type: ignore


# -----------------------------
# Ti·ªán √≠ch x·ª≠ l√Ω vƒÉn b·∫£n
# -----------------------------

def normalize_text(text: Optional[str]) -> str:
    if not text:
        return ""
    # Thay '_' do tokenizer th√†nh ' ', c·∫Øt kho·∫£ng tr·∫Øng
    return text.replace("_", " ").strip()


def jaccard_similarity(a: str, b: str) -> float:
    """Jaccard ƒë∆°n gi·∫£n (token set)."""
    if not a or not b:
        return 0.0
    set_a = set(a.lower().split())
    set_b = set(b.lower().split())
    if not set_a or not set_b:
        return 0.0
    inter = len(set_a & set_b)
    union = len(set_a | set_b)
    if union == 0:
        return 0.0
    return inter / union


# -----------------------------
# ƒê·ªçc d·ªØ li·ªáu ƒë·∫ßu v√†o linh ho·∫°t
# -----------------------------

def _read_json_or_jsonl(path: Path) -> Iterable[Dict]:
    """ƒê·ªçc file JSON (list) ho·∫∑c JSONL (m·ªói d√≤ng m·ªôt JSON)."""
    if not path.exists():
        return []
    if path.suffix.lower() in {".jsonl", ".jsonl.gz"}:
        # ƒê∆°n gi·∫£n: kh√¥ng x·ª≠ l√Ω gz ·ªü ƒë√¢y ƒë·ªÉ tr√°nh ph·ª• thu·ªôc
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except Exception:
                    continue
    else:
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            if isinstance(data, list):
                for obj in data:
                    if isinstance(obj, dict):
                        yield obj
            elif isinstance(data, dict):
                # M·ªôt s·ªë b·ªô c√≥ d·∫°ng {"data":[...]}
                items = data.get("data") or data.get("examples") or []
                if isinstance(items, list):
                    for obj in items:
                        if isinstance(obj, dict):
                            yield obj
        except Exception:
            return []


def _extract_query(obj: Dict) -> Optional[str]:
    for key in ("query", "question", "q", "prompt"):
        val = obj.get(key)
        if isinstance(val, str) and val.strip():
            return normalize_text(val)
    return None


def _extract_positive_texts(obj: Dict) -> List[str]:
    """C·ªë g·∫Øng l·∫•y positive passages t·ª´ nhi·ªÅu schema kh√°c nhau.

    ∆Øu ti√™n: context/positive_ctxs ‚Üí text; n·∫øu kh√¥ng c√≥, tr·∫£ []
    """
    positives: List[str] = []

    # Tr∆∞·ªùng ƒë∆°n l·∫ª
    for key in ("context", "positive", "positive_text"):
        val = obj.get(key)
        if isinstance(val, str) and val.strip():
            positives.append(normalize_text(val))

    # Danh s√°ch passages
    for key in ("positive_ctxs", "positive_passages", "ctxs", "passages", "contexts"):
        arr = obj.get(key)
        if isinstance(arr, list):
            for item in arr:
                if isinstance(item, dict):
                    # C√°c t√™n tr∆∞·ªùng kh·∫£ dƒ©
                    txt = item.get("text") or item.get("content") or item.get("passage")
                    # M·ªôt s·ªë dataset ƒë√°nh d·∫•u positive
                    flag = item.get("is_positive") or item.get("label") == "positive"
                    if txt and (flag or key in ("positive_ctxs", "positive_passages")):
                        positives.append(normalize_text(str(txt)))
    # Lo·∫°i r·ªóng & tr√πng
    positives = [p for p in positives if p]
    if positives:
        # C·∫Øt ng·∫Øn ƒë·ªÉ tr√°nh ƒëo·∫°n qu√° d√†i
        positives = [p[:4000] for p in positives]
    return positives


@dataclass
class Example:
    query: str
    positive_text: Optional[str]
    source: str
    positive_id: Optional[int] = None  # H·ªó tr·ª£ ƒë·∫ßu v√†o ƒë√£ c√≥ s·∫µn gold chunk id


def iter_examples_from_path(path: Path) -> Iterable[Example]:
    source_name = path.stem
    for obj in _read_json_or_jsonl(path):
        q = _extract_query(obj)
        if not q:
            continue
        positives = _extract_positive_texts(obj)
        # Th·ª≠ ƒë·ªçc s·∫µn positive_id/gold_chunk_id n·∫øu c√≥
        pos_id: Optional[int] = None
        for key in ("positive_id", "gold_chunk_id", "gold_id", "positive_chunk_id"):
            if key in obj:
                try:
                    pos_id = int(obj.get(key))  # type: ignore
                    break
                except Exception:
                    pos_id = None
        
        if positives:
            # C√≥ th·ªÉ l·∫•y nhi·ªÅu positives; ƒë·ªÉ c√¢n b·∫±ng, ch·ªâ l·∫•y 1 c√°i ƒë·∫ßu
            yield Example(query=q, positive_text=positives[0], source=source_name, positive_id=pos_id)
        else:
            # Tr∆∞·ªùng h·ª£p ch·ªâ c√≥ query: ƒë·ªÉ None, s·∫Ω √°nh x·∫° positive b·∫±ng query
            yield Example(query=q, positive_text=None, source=source_name, positive_id=pos_id)


def auto_find_raw_files(raw_dir: Path) -> List[Path]:
    candidates: List[Path] = []
    if not raw_dir.exists():
        return candidates
    names = [
        "VNLAWQC.jsonl", "VNLAWQC.json",
        "VNSynLawQC.jsonl", "VNSynLawQC.json",
        "vlqa.json", "vlqa.jsonl",
    ]
    lower_to_path: Dict[str, Path] = {}
    for p in raw_dir.glob("**/*"):
        if p.suffix.lower() not in {".json", ".jsonl"}:
            continue
        lower_to_path[p.name.lower()] = p
    for n in names:
        p = lower_to_path.get(n.lower())
        if p:
            candidates.append(p)
    return candidates


# -----------------------------
# √Ånh x·∫° passage ‚Üí chunk_id b·∫±ng FAISS
# -----------------------------

class PositiveMapper:
    def __init__(self, use_gpu: bool = False) -> None:
        self.retriever = RetrievalService(use_gpu=use_gpu)

    def map_text_to_chunk(self, text: str, dense_top_k: int = 8) -> Optional[Tuple[int, float]]:
        """D√πng encoder + FAISS ƒë·ªÉ t√¨m chunk g·∫ßn nh·∫•t cho m·ªôt ƒëo·∫°n vƒÉn ho·∫∑c ch√≠nh query."""
        text = normalize_text(text)
        if not text:
            return None
        try:
            query_vec = self.retriever.encode_query(text)
            k = max(1, min(dense_top_k, int(self.retriever.index.ntotal)))
            distances, indices = self.retriever.index.search(query_vec, k)
            if k <= 0 or len(indices[0]) == 0:
                return None
            # Ch·ªçn ph·∫ßn t·ª≠ t·ªët nh·∫•t (gi√° tr·ªã distance cao nh·∫•t v√¨ d√πng IP + normalized)
            best_idx = int(indices[0][0])
            best_score = float(distances[0][0])
            return best_idx, best_score
        except Exception:
            return None

    def get_hard_negatives(self, query: str, positive_id: Optional[int], top_k: int = 32, max_return: int = 15) -> List[int]:
        results = self.retriever.retrieve(query, top_k=top_k)
        ids: List[int] = []
        for r in results:
            cid = int(r.get("chunk_id")) if r.get("chunk_id") is not None else -1
            if cid < 0:
                continue
            if positive_id is not None and cid == positive_id:
                continue
            ids.append(cid)
            if len(ids) >= max_return:
                break
        return ids


# -----------------------------
# Ghi JSONL
# -----------------------------

def write_jsonl(output_path: Path, records: Iterable[Dict]) -> int:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    n = 0
    with output_path.open("w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            n += 1
    return n


# -----------------------------
# Main pipeline
# -----------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="Chu·∫©n h√≥a d·ªØ li·ªáu retriever ‚Üí retrieval_train.jsonl")
    parser.add_argument("--input", nargs="*", default=[], help="ƒê∆∞·ªùng d·∫´n file JSON/JSONL (VNLAWQC, VNSynLawQC)")
    parser.add_argument("--output", default=str(get_processed_data_dir() / "retrieval_train.jsonl"), help="File JSONL ƒë·∫ßu ra")
    parser.add_argument("--dense-top-k", type=int, default=8, help="TopK khi √°nh x·∫° positive_text ‚Üí chunk_id")
    parser.add_argument("--hard-negatives", type=int, default=15, help="S·ªë l∆∞·ª£ng hard negatives m·ªói v√≠ d·ª•")
    parser.add_argument("--hn-top-k", type=int, default=64, help="TopK truy h·ªìi ƒë·ªÉ r√∫t hard negatives")
    parser.add_argument("--min-positive-score", type=float, default=0.20, help="Ng∆∞·ª°ng distance t·ªëi thi·ªÉu ƒë·ªÉ ch·∫•p nh·∫≠n positive mapping")
    parser.add_argument("--skip-jaccard", action="store_true", help="B·ªè qua ki·ªÉm tra Jaccard khi positive ƒë∆∞·ª£c map t·ª± ƒë·ªông (tƒÉng t·ªëc)")
    parser.add_argument("--limit", type=int, default=0, help="Gi·ªõi h·∫°n s·ªë m·∫´u/t·∫≠p (0 = kh√¥ng gi·ªõi h·∫°n)")
    parser.add_argument("--use-gpu", action="store_true", help="D√πng GPU n·∫øu c√≥ s·∫µn")
    args = parser.parse_args()

    project_root = get_project_root()
    raw_dir = project_root / "data" / "raw"

    # T√¨m input n·∫øu kh√¥ng truy·ªÅn
    input_paths: List[Path] = [Path(p) for p in args.input]
    if not input_paths:
        input_paths = auto_find_raw_files(raw_dir)

    if not input_paths:
        print("‚ùå Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ file ƒë·∫ßu v√†o n√†o trong --input ho·∫∑c data/raw/.")
        print("   Vui l√≤ng ch·ªâ ƒë·ªãnh --input c√°c file VNLAWQC/VNSynLawQC.")
        sys.exit(1)

    print("üöÄ Chu·∫©n h√≥a d·ªØ li·ªáu retriever...")
    print(f"üìÇ Input files: {', '.join(str(p) for p in input_paths)}")
    print(f"üíæ Output: {args.output}")

    # Kh·ªüi t·∫°o mapper s·ª≠ d·ª•ng FAISS hi·ªán t·∫°i
    mapper = PositiveMapper(use_gpu=args.use_gpu)

    # T·∫°o generator ghi JSONL d·∫ßn d·∫ßn
    def gen_records() -> Iterable[Dict]:
        for path in input_paths:
            count = 0
            for ex in iter_examples_from_path(path):
                # N·∫øu input ƒë√£ c√≥ s·∫µn positive_id th√¨ d√πng lu√¥n (tƒÉng t·ªëc c·ª±c m·∫°nh)
                if ex.positive_id is not None:
                    positive_id = int(ex.positive_id)
                    positive_score = 1.0
                else:
                    # √Ånh x·∫° positive: n·∫øu c√≥ positive_text d√πng n√≥, n·∫øu kh√¥ng d√πng ch√≠nh query
                    basis_text = ex.positive_text if ex.positive_text else ex.query
                    mapped = mapper.map_text_to_chunk(basis_text, dense_top_k=args.dense_top_k)
                    if not mapped:
                        continue
                    positive_id, positive_score = mapped

                    # Ki·ªÉm tra ng∆∞·ª°ng (ch·ªâ √°p d·ª•ng khi ph·∫£i t·ª± map)
                    if positive_score < args.min_positive_score and not args.skip_jaccard:
                        # Th·ª≠ m·ªôt th·ªß thu·∫≠t: so Jaccard v·ªõi content top1 ƒë·ªÉ l·ªçc nh·∫π
                        top_content = mapper.retriever.get_chunk_content(positive_id) or ""
                        if jaccard_similarity(ex.positive_text, top_content) < 0.08:
                            continue

                # Sinh hard negatives theo truy v·∫•n g·ªëc
                hard_negs = mapper.get_hard_negatives(
                    query=ex.query,
                    positive_id=positive_id,
                    top_k=args.hn_top_k,
                    max_return=args.hard_negatives,
                )

                yield {
                    "query": ex.query,
                    "positive_id": int(positive_id),
                    "hard_negatives": [int(x) for x in hard_negs],
                    "positive_score": float(positive_score),
                    "source": ex.source,
                }

                count += 1
                if args.limit and count >= int(args.limit):
                    break

    out_path = Path(args.output)
    total = write_jsonl(out_path, gen_records())

    print("‚úÖ Ho√†n th√†nh chu·∫©n h√≥a d·ªØ li·ªáu retriever!")
    print(f"üìä S·ªë d√≤ng ghi: {total}")
    print(f"üìÅ File: {out_path}")


if __name__ == "__main__":
    main()


