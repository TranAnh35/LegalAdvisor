#!/usr/bin/env python3
"""
Script táº¡o FAISS index tá»« document chunks (chuáº©n Zalo-AI-Legal, khÃ´ng cÃ²n tÆ°Æ¡ng thÃ­ch dá»¯ liá»‡u cÅ©)
"""
import os
import json
from pathlib import Path
import numpy as np
from sentence_transformers import SentenceTransformer, models as st_models
import faiss
from tqdm import tqdm
import torch

def load_document_chunks():
    """Chá»‰ Ä‘á»c JSONL schema má»›i Zalo-AI-Legal"""
    jsonl_path = Path(__file__).parent.parent.parent / "data" / "processed" / "zalo-legal" / "chunks_schema.jsonl"
    if not jsonl_path.exists():
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {jsonl_path}")
        return None, None, None
    chunks = []
    with open(jsonl_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    chunks.append(json.loads(line))
                except Exception as e:
                    print(f"Lá»—i parse JSONL: {e}")
    print(f"Sá»‘ lÆ°á»£ng chunks: {len(chunks)}")
    # For embedding: láº¥y content, náº¿u muá»‘n thÃ¬ cá»™ng thÃªm title/sá»‘ hiá»‡u
    texts = [chunk.get('content', '') or '' for chunk in chunks]
    ids = [int(chunk.get('chunk_id')) for chunk in chunks]
    metadata = [{
        'chunk_id': chunk.get('chunk_id'),
        'corpus_id': chunk.get('corpus_id'),
        'type': chunk.get('type'),
        'number': chunk.get('number'),
        'year': chunk.get('year'),
        'suffix': chunk.get('suffix'),
        'word_count': chunk.get('word_count'),
        'preview': chunk.get('preview')
    } for chunk in chunks]
    return texts, metadata, ids

def create_embeddings(texts, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
    """Táº¡o embeddings cho texts"""
    print(f"ğŸ¤– Load model: {model_name}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  Device: {device}")
    print(f"ğŸ§  Embedding model: {model_name}")
    model = SentenceTransformer(model_name, device=device)
    print("ğŸ”„ Táº¡o embeddings...")
    batch_size = 256
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, normalize_embeddings=False)
        embeddings.append(batch_embeddings)
    embeddings = np.vstack(embeddings)
    print(f"ğŸ“Š Embeddings shape: {embeddings.shape}")
    return embeddings, model

def build_faiss_index(embeddings, ids=None):
    print("ğŸ—ï¸ XÃ¢y dá»±ng FAISS index...")
    dimension = embeddings.shape[1]
    faiss.normalize_L2(embeddings)
    # DÃ¹ng IndexFlatIP cho cosine similarity
    base_index = faiss.IndexFlatIP(dimension)
    if ids is not None:
        index = faiss.IndexIDMap(base_index)
        index.add_with_ids(embeddings, np.asarray(ids, dtype=np.int64))
    else:
        index = base_index
        index.add(embeddings)
    print(f"âœ… FAISS index created with {index.ntotal} vectors")
    return index

def save_index_and_metadata(index, metadata, model, output_dir="../models/retrieval"):
    output_dir = Path(__file__).parent.parent.parent / "models" / "retrieval"
    output_dir.mkdir(parents=True, exist_ok=True)
    index_path = output_dir / "faiss_index.bin"
    faiss.write_index(index, str(index_path))
    print(f"ğŸ’¾ FAISS index saved: {index_path}")
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Metadata saved: {metadata_path}")
    model_info = {
        "model_name": getattr(model, "model_card", None) or os.getenv("LEGALADVISOR_EMB_MODEL") or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "embedding_dim": index.d,
        "num_chunks": index.ntotal
    }
    model_info_path = output_dir / "model_info.json"
    with open(model_info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Model info saved: {model_info_path}")

def main():
    print("ğŸš€ Báº¯t Ä‘áº§u táº¡o FAISS index cho retrieval tá»« Zalo-AI-Legal ...")
    texts, metadata, ids = load_document_chunks()
    if texts is None:
        return
    embeddings, model = create_embeddings(texts)
    index = build_faiss_index(embeddings, ids=ids)
    save_index_and_metadata(index, metadata, model)
    print("\nâœ… HoÃ n thÃ nh táº¡o FAISS index má»›i!")
    print("ğŸ“ CÃ¡c file Ä‘Æ°á»£c lÆ°u táº¡i: ../models/retrieval/")
    print("   - faiss_index.bin: FAISS index")
    print("   - metadata.json: ThÃ´ng tin chunks")
    print("   - model_info.json: ThÃ´ng tin model")

if __name__ == "__main__":
    main()
