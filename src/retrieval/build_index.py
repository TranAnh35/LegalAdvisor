#!/usr/bin/env python3
"""
Script táº¡o FAISS index tá»« document chunks
"""

import os
import sys
sys.path.append('../..')

import json
from pathlib import Path
from src.utils.paths import get_processed_data_dir, get_project_root
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm
import torch

def load_document_chunks():
    """Load document chunks.

    Æ¯u tiÃªn: Parquet â†’ SQLite (khÃ´ng cÃ²n dÃ¹ng JSON)
    """
    # XÃ¡c Ä‘á»‹nh thÆ° má»¥c data/processed theo util
    base_dir = get_processed_data_dir()

    # 1) Parquet
    parquet_file = base_dir / "smart_chunks_stable.parquet"
    if parquet_file.exists():
        try:
            import pandas as pd  # type: ignore
            print(f"ğŸ“– Äá»c Parquet: {parquet_file}")
            df = pd.read_parquet(parquet_file)
            chunks = df.to_dict(orient="records")
        except Exception as e:
            print(f"âš ï¸  Lá»—i Ä‘á»c Parquet ({e}). Thá»­ SQLite/JSON...")
            chunks = None
    else:
        chunks = None

    # 2) SQLite
    if chunks is None:
        sqlite_file = base_dir / "smart_chunks_stable.db"
        if sqlite_file.exists():
            try:
                import sqlite3
                print(f"ğŸ“– Äá»c SQLite: {sqlite_file}")
                conn = sqlite3.connect(str(sqlite_file))
                cur = conn.cursor()
                cur.execute(
                    """
                    SELECT chunk_id, doc_file, doc_title, chapter, section, article,
                           article_heading, clause, point, chunk_index, content,
                           word_count, chunk_type
                    FROM chunks
                    ORDER BY chunk_id
                    """
                )
                rows = cur.fetchall()
                cols = [
                    'chunk_id', 'doc_file', 'doc_title', 'chapter', 'section', 'article',
                    'article_heading', 'clause', 'point', 'chunk_index', 'content',
                    'word_count', 'chunk_type'
                ]
                chunks = [dict(zip(cols, row)) for row in rows]
                conn.close()
            except Exception as e:
                print(f"âš ï¸  Lá»—i Ä‘á»c SQLite ({e}).")
                chunks = None
    # KhÃ´ng cÃ²n fallback JSON
    if chunks is None:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u chunks trong Parquet/SQLite!")
        return None, None

    print(f"ğŸ“Š Tá»•ng sá»‘ chunks: {len(chunks)}")

    # Láº¥y ná»™i dung chunks vÃ  ids á»•n Ä‘á»‹nh
    # ViTokenizer cÃ³ thá»ƒ táº¡o dáº¥u '_' ná»‘i tá»«; chuáº©n hÃ³a vá» khoáº£ng tráº¯ng Ä‘á»ƒ cáº£i thiá»‡n embedding
    def normalize_for_embedding(text: str) -> str:
        return (text or '').replace('_', ' ').strip()

    # Giáº£m kÃ­ch thÆ°á»›c báº±ng cÃ¡ch cáº¯t content input embedding (vÃ­ dá»¥ 800 tokens ~ 4k chars)
    texts = [normalize_for_embedding((chunk.get('content', '') or '')[:4000]) for chunk in chunks]
    ids = [int(chunk.get('chunk_id')) if chunk.get('chunk_id') is not None else idx for idx, chunk in enumerate(chunks)]
    metadata = [{
        'chunk_id': chunk.get('chunk_id'),
        'doc_file': chunk.get('doc_file'),
        # TrÃ¡nh phÃ¬nh metadata.json: cáº¯t tiÃªu Ä‘á» tá»‘i Ä‘a 200 kÃ½ tá»±
        'doc_title': (chunk.get('doc_title')[:200] if isinstance(chunk.get('doc_title'), str) else chunk.get('doc_title')),
        'chunk_index': chunk.get('chunk_index'),
        'word_count': chunk.get('word_count'),
        'chapter': chunk.get('chapter'),
        'section': chunk.get('section'),
        'article': chunk.get('article'),
        'article_heading': chunk.get('article_heading'),
        'clause': chunk.get('clause'),
        'point': chunk.get('point'),
        'chunk_type': chunk.get('chunk_type'),
        # Preview ngáº¯n Ä‘á»ƒ giá»¯ dung lÆ°á»£ng
        'preview': normalize_for_embedding(chunk.get('content', ''))[:200]
    } for chunk in chunks]

    return texts, metadata, ids

def create_embeddings(texts, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
    """Táº¡o embeddings cho texts"""

    print(f"ğŸ¤– Load model: {model_name}")

    # Load model
    use_gpu_env = os.getenv("LEGALADVISOR_USE_GPU", "auto").lower()
    if use_gpu_env == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    elif use_gpu_env in ("1", "true", "yes", "on", "cuda", "gpu"):
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = "cpu"

    print(f"ğŸ–¥ï¸  Device: {device}")
    model = SentenceTransformer(model_name, device=device)

    print("ğŸ”„ Táº¡o embeddings...")

    # Táº¡o embeddings theo batch Ä‘á»ƒ trÃ¡nh memory error
    batch_size = int(os.getenv("LEGALADVISOR_EMB_BATCH", "256"))
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, normalize_embeddings=False)
        embeddings.append(batch_embeddings)

    # GhÃ©p táº¥t cáº£ embeddings
    embeddings = np.vstack(embeddings)

    print(f"ğŸ“Š Embeddings shape: {embeddings.shape}")

    return embeddings, model

def build_faiss_index(embeddings, ids=None):
    """Build FAISS index tá»« embeddings.

    Náº¿u cung cáº¥p ids (chunk_id), sáº½ sá»­ dá»¥ng IndexIDMap Ä‘á»ƒ Ã¡nh xáº¡ á»•n Ä‘á»‹nh.
    """

    print("ğŸ—ï¸ XÃ¢y dá»±ng FAISS index...")

    # Láº¥y dimension cá»§a embeddings
    dimension = embeddings.shape[1]

    # Táº¡o FAISS index vá»›i Inner Product (cho cosine similarity)
    base_index = faiss.IndexFlatIP(dimension)

    # Normalize embeddings cho cosine similarity
    faiss.normalize_L2(embeddings)

    # Add vectors
    if ids is not None:
        # Bá»c vá»›i IDMap vÃ  add kÃ¨m ids (int64)
        index = faiss.IndexIDMap(base_index)
        ids_array = np.asarray(ids, dtype=np.int64)
        index.add_with_ids(embeddings, ids_array)
    else:
        index = base_index
        index.add(embeddings)

    print(f"âœ… FAISS index created with {index.ntotal} vectors")

    return index

def save_index_and_metadata(index, metadata, model, output_dir="../models/retrieval", used_id_map=True):
    """LÆ°u FAISS index vÃ  metadata"""

    # LuÃ´n lÆ°u vá» thÆ° má»¥c models/retrieval táº¡i gá»‘c dá»± Ã¡n
    project_root = get_project_root()
    output_dir = project_root / "models" / "retrieval"
    output_dir.mkdir(parents=True, exist_ok=True)

    # LÆ°u FAISS index
    index_path = output_dir / "faiss_index.bin"
    faiss.write_index(index, str(index_path))
    print(f"ğŸ’¾ FAISS index saved: {index_path}")

    # LÆ°u metadata
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Metadata saved: {metadata_path}")

    # LÆ°u model info
    model_info = {
        "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "embedding_dim": index.d,
        "num_chunks": index.ntotal,
        "uses_id_map": bool(used_id_map)
    }

    model_info_path = output_dir / "model_info.json"
    with open(model_info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Model info saved: {model_info_path}")

def main():
    """HÃ m chÃ­nh"""

    print("ğŸš€ Báº¯t Ä‘áº§u táº¡o FAISS index cho retrieval...")

    # Load document chunks
    loaded = load_document_chunks()
    if loaded is None or loaded[0] is None:
        return
    texts, metadata, ids = loaded

    # Táº¡o embeddings
    embeddings, model = create_embeddings(texts)

    # Build FAISS index
    index = build_faiss_index(embeddings, ids=ids)

    # LÆ°u index vÃ  metadata
    save_index_and_metadata(index, metadata, model, used_id_map=True)

    print("\nâœ… HoÃ n thÃ nh táº¡o FAISS index!")
    print("ğŸ“ CÃ¡c file Ä‘Æ°á»£c lÆ°u táº¡i: ../models/retrieval/")
    print("   - faiss_index.bin: FAISS index")
    print("   - metadata.json: ThÃ´ng tin chunks")
    print("   - model_info.json: ThÃ´ng tin model")

if __name__ == "__main__":
    main()
