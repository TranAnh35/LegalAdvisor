#!/usr/bin/env python3
"""
Script t·∫°o FAISS index t·ª´ document chunks
"""

import os
import sys
sys.path.append('../..')

import json
from pathlib import Path
from src.utils.paths import get_processed_data_dir, get_project_root
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from tqdm import tqdm
import torch

def load_document_chunks():
    """Load document chunks.

    ∆Øu ti√™n: Parquet ‚Üí SQLite (kh√¥ng c√≤n d√πng JSON)
    """
    # X√°c ƒë·ªãnh th∆∞ m·ª•c data/processed theo util
    base_dir = get_processed_data_dir()

    # 1) Parquet
    parquet_file = base_dir / "smart_chunks_stable.parquet"
    if parquet_file.exists():
        try:
            import pandas as pd  # type: ignore
            print(f"üìñ ƒê·ªçc Parquet: {parquet_file}")
            df = pd.read_parquet(parquet_file)
            chunks = df.to_dict(orient="records")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói ƒë·ªçc Parquet ({e}). Th·ª≠ SQLite/JSON...")
            chunks = None
    else:
        chunks = None

    # 2) SQLite
    if chunks is None:
        sqlite_file = base_dir / "smart_chunks_stable.db"
        if sqlite_file.exists():
            try:
                import sqlite3
                print(f"üìñ ƒê·ªçc SQLite: {sqlite_file}")
                conn = sqlite3.connect(str(sqlite_file))
                cur = conn.cursor()
                # C·ªë g·∫Øng ƒë·ªçc th√™m c√°c c·ªôt metadata m·ªü r·ªông n·∫øu t·ªìn t·∫°i
                query_extended = (
                    """
                    SELECT chunk_id, doc_file, doc_title, chapter, section, article,
                           article_heading, clause, point, chunk_index, content,
                           word_count, chunk_type,
                           effective_date, effective_year, promulgation_date, promulgation_year, citations
                    FROM chunks
                    ORDER BY chunk_id
                    """
                )
                query_basic = (
                    """
                    SELECT chunk_id, doc_file, doc_title, chapter, section, article,
                           article_heading, clause, point, chunk_index, content,
                           word_count, chunk_type
                    FROM chunks
                    ORDER BY chunk_id
                    """
                )
                try:
                    cur.execute(query_extended)
                    rows = cur.fetchall()
                    cols = [
                        'chunk_id', 'doc_file', 'doc_title', 'chapter', 'section', 'article',
                        'article_heading', 'clause', 'point', 'chunk_index', 'content',
                        'word_count', 'chunk_type', 'effective_date', 'effective_year',
                        'promulgation_date', 'promulgation_year', 'citations'
                    ]
                except Exception:
                    cur.execute(query_basic)
                    rows = cur.fetchall()
                    cols = [
                        'chunk_id', 'doc_file', 'doc_title', 'chapter', 'section', 'article',
                        'article_heading', 'clause', 'point', 'chunk_index', 'content',
                        'word_count', 'chunk_type'
                    ]
                rows = cur.fetchall()
                chunks = [dict(zip(cols, row)) for row in rows]
                conn.close()
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói ƒë·ªçc SQLite ({e}).")
                chunks = None
    # Kh√¥ng c√≤n fallback JSON
    if chunks is None:
        print("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu chunks trong Parquet/SQLite!")
        return None, None

    print(f"üìä T·ªïng s·ªë chunks: {len(chunks)}")

    # L·∫•y n·ªôi dung chunks v√† ids ·ªïn ƒë·ªãnh
    # ViTokenizer c√≥ th·ªÉ t·∫°o d·∫•u '_' n·ªëi t·ª´; chu·∫©n h√≥a v·ªÅ kho·∫£ng tr·∫Øng ƒë·ªÉ c·∫£i thi·ªán embedding
    def normalize_for_embedding(text: str) -> str:
        return (text or '').replace('_', ' ').strip()

    # Gi·∫£m k√≠ch th∆∞·ªõc b·∫±ng c√°ch c·∫Øt content input embedding (v√≠ d·ª• 800 tokens ~ 4k chars)
    texts = [normalize_for_embedding((chunk.get('content', '') or '')[:4000]) for chunk in chunks]
    ids = [int(chunk.get('chunk_id')) if chunk.get('chunk_id') is not None else idx for idx, chunk in enumerate(chunks)]
    metadata = [{
        'chunk_id': chunk.get('chunk_id'),
        'doc_file': chunk.get('doc_file'),
        # Tr√°nh ph√¨nh metadata.json: c·∫Øt ti√™u ƒë·ªÅ t·ªëi ƒëa 200 k√Ω t·ª±
        'doc_title': (chunk.get('doc_title')[:200] if isinstance(chunk.get('doc_title'), str) else chunk.get('doc_title')),
        'chunk_index': chunk.get('chunk_index'),
        'word_count': chunk.get('word_count'),
        'chapter': chunk.get('chapter'),
        'section': chunk.get('section'),
        'article': chunk.get('article'),
        'article_heading': chunk.get('article_heading'),
        'clause': chunk.get('clause'),
        'point': chunk.get('point'),
        'chunk_type': chunk.get('chunk_type'),
        # Metadata m·ªü r·ªông n·∫øu c√≥
        'effective_date': chunk.get('effective_date'),
        'effective_year': chunk.get('effective_year'),
        'promulgation_date': chunk.get('promulgation_date'),
        'promulgation_year': chunk.get('promulgation_year'),
        'citations': chunk.get('citations'),
        # Preview ng·∫Øn ƒë·ªÉ gi·ªØ dung l∆∞·ª£ng
        'preview': normalize_for_embedding(chunk.get('content', ''))[:200]
    } for chunk in chunks]

    return texts, metadata, ids

def create_embeddings(texts, model_name: str, batch_size: int, device: str):
    """T·∫°o embeddings cho texts"""

    print(f"ü§ñ Load model: {model_name}")

    # Chu·∫©n h√≥a & ki·ªÉm tra thi·∫øt b·ªã
    requested_device = (device or "auto").lower()
    if requested_device == "auto":
        effective_device = "cuda" if torch.cuda.is_available() else "cpu"
    elif requested_device == "cuda":
        if not torch.cuda.is_available():
            print(
                f"‚ùå ƒê√£ y√™u c·∫ßu CUDA nh∆∞ng torch.cuda.is_available()=False. Vui l√≤ng ch·∫°y trong m√¥i tr∆∞·ªùng GPU."
            )
            raise RuntimeError("CUDA requested but not available")
        effective_device = "cuda"
    elif requested_device == "cpu":
        effective_device = "cpu"
    else:
        # ch·∫•p nh·∫≠n c√°c alias ph·ªï bi·∫øn
        if requested_device in ("1", "true", "yes", "on", "gpu"):
            effective_device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            effective_device = "cpu"

    try:
        gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
    except Exception:
        gpu_name = "N/A"

    print(
        f"üñ•Ô∏è  Device requested: {requested_device} | cuda_available={torch.cuda.is_available()} | "
        f"num_devices={torch.cuda.device_count()} | using={effective_device} | gpu0={gpu_name}"
    )
    model = SentenceTransformer(model_name, device=effective_device)

    print("üîÑ T·∫°o embeddings...")

    # T·∫°o embeddings theo batch ƒë·ªÉ tr√°nh memory error
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(
            batch_texts,
            convert_to_numpy=True,
            normalize_embeddings=False,
            device=effective_device,
        )
        embeddings.append(batch_embeddings)

    # Gh√©p t·∫•t c·∫£ embeddings
    embeddings = np.vstack(embeddings)

    print(f"üìä Embeddings shape: {embeddings.shape}")

    return embeddings, model, effective_device

def build_faiss_index(embeddings, ids=None):
    """Build FAISS index t·ª´ embeddings.

    N·∫øu cung c·∫•p ids (chunk_id), s·∫Ω s·ª≠ d·ª•ng IndexIDMap ƒë·ªÉ √°nh x·∫° ·ªïn ƒë·ªãnh.
    """

    print("üèóÔ∏è X√¢y d·ª±ng FAISS index...")

    # L·∫•y dimension c·ªßa embeddings
    dimension = embeddings.shape[1]

    # T·∫°o FAISS index v·ªõi Inner Product (cho cosine similarity)
    base_index = faiss.IndexFlatIP(dimension)

    # Normalize embeddings cho cosine similarity
    faiss.normalize_L2(embeddings)

    # Add vectors
    if ids is not None:
        # B·ªçc v·ªõi IDMap v√† add k√®m ids (int64)
        index = faiss.IndexIDMap(base_index)
        ids_array = np.asarray(ids, dtype=np.int64)
        index.add_with_ids(embeddings, ids_array)
    else:
        index = base_index
        index.add(embeddings)

    print(f"‚úÖ FAISS index created with {index.ntotal} vectors")

    return index

def save_index_and_metadata(index, metadata, model_name: str, emb_batch: int, device: str, output_dir="../models/retrieval", used_id_map=True):
    """L∆∞u FAISS index v√† metadata"""

    # Lu√¥n l∆∞u v·ªÅ th∆∞ m·ª•c models/retrieval t·∫°i g·ªëc d·ª± √°n
    project_root = get_project_root()
    output_dir = project_root / "models" / "retrieval"
    output_dir.mkdir(parents=True, exist_ok=True)

    # L∆∞u FAISS index
    index_path = output_dir / "faiss_index.bin"
    faiss.write_index(index, str(index_path))
    print(f"üíæ FAISS index saved: {index_path}")

    # L∆∞u metadata
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"üíæ Metadata saved: {metadata_path}")

    # L∆∞u model info
    model_info = {
        "model_name": model_name,
        "embedding_dim": index.d,
        "num_chunks": index.ntotal,
        "uses_id_map": bool(used_id_map),
        "batch_size": emb_batch,
        "device": device,
    }

    model_info_path = output_dir / "model_info.json"
    with open(model_info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    print(f"üíæ Model info saved: {model_info_path}")

def main():
    """H√†m ch√≠nh"""

    print("üöÄ B·∫Øt ƒë·∫ßu t·∫°o FAISS index cho retrieval...")

    # CLI args
    import argparse
    parser = argparse.ArgumentParser(description="Build FAISS index for LegalAdvisor")
    parser.add_argument("--model", type=str, default=None, help="T√™n model HF ho·∫∑c ƒë∆∞·ªùng d·∫´n local ƒë·∫øn SentenceTransformer ƒë√£ fine-tune")
    parser.add_argument("--emb-batch", type=int, default=None, help="Batch size khi t·∫°o embedding (m·∫∑c ƒë·ªãnh t·ª´ env LEGALADVISOR_EMB_BATCH ho·∫∑c 256)")
    parser.add_argument("--device", type=str, default=None, choices=["auto", "cpu", "cuda"], help="Thi·∫øt b·ªã encode: auto/cpu/cuda (m·∫∑c ƒë·ªãnh auto ho·∫∑c t·ª´ LEGALADVISOR_USE_GPU)")
    args = parser.parse_args()

    # Load document chunks
    loaded = load_document_chunks()
    if loaded is None or loaded[0] is None:
        return
    texts, metadata, ids = loaded

    # T·∫°o embeddings
    # Resolve model name
    model_name = (
        args.model
        or os.getenv("LEGALADVISOR_EMB_MODEL")
        or "intfloat/multilingual-e5-small"
    )
    # Resolve batch size
    emb_batch = args.emb_batch if args.emb_batch is not None else int(os.getenv("LEGALADVISOR_EMB_BATCH", "256"))
    # Resolve device
    device = args.device if args.device is not None else os.getenv("LEGALADVISOR_USE_GPU", "auto").lower()

    embeddings, _, effective_device = create_embeddings(
        texts, model_name=model_name, batch_size=emb_batch, device=device
    )

    # Build FAISS index
    index = build_faiss_index(embeddings, ids=ids)

    # L∆∞u index v√† metadata
    save_index_and_metadata(
        index,
        metadata,
        model_name=model_name,
        emb_batch=emb_batch,
        device=effective_device,
        used_id_map=True,
    )

    print("\n‚úÖ Ho√†n th√†nh t·∫°o FAISS index!")
    print("üìÅ C√°c file ƒë∆∞·ª£c l∆∞u t·∫°i: ../models/retrieval/")
    print("   - faiss_index.bin: FAISS index")
    print("   - metadata.json: Th√¥ng tin chunks")
    print("   - model_info.json: Th√¥ng tin model")

if __name__ == "__main__":
    main()
