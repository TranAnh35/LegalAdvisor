#!/usr/bin/env python3
"""
Script t·∫°o FAISS index t·ª´ document chunks
"""

import os
import sys
sys.path.append('../..')

from dotenv import load_dotenv
load_dotenv() 

import json
from pathlib import Path
from src.utils.paths import get_processed_data_dir, get_project_root
import numpy as np
from sentence_transformers import SentenceTransformer
from sentence_transformers import models as st_models
import faiss
from tqdm import tqdm
import torch
from dotenv import load_dotenv

# N·∫°p bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env (n·∫øu c√≥)
load_dotenv()

def load_document_chunks():
    """Load document chunks.

    ∆Øu ti√™n: Parquet ‚Üí SQLite (kh√¥ng c√≤n d√πng JSON)
    """
    # X√°c ƒë·ªãnh th∆∞ m·ª•c data/processed theo util
    base_dir = get_processed_data_dir()

    # 1) Parquet
    parquet_file = base_dir / "smart_chunks_stable.parquet"
    if parquet_file.exists():
        try:
            import pandas as pd  # type: ignore
            print(f"üìñ ƒê·ªçc Parquet: {parquet_file}")
            df = pd.read_parquet(parquet_file)
            chunks = df.to_dict(orient="records")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói ƒë·ªçc Parquet ({e}). Th·ª≠ SQLite/JSON...")
            chunks = None
    else:
        chunks = None

    # 2) SQLite
    if chunks is None:
        sqlite_file = base_dir / "smart_chunks_stable.db"
        if sqlite_file.exists():
            try:
                import sqlite3
                print(f"üìñ ƒê·ªçc SQLite: {sqlite_file}")
                conn = sqlite3.connect(str(sqlite_file))
                cur = conn.cursor()
                cur.execute(
                    """
                    SELECT chunk_id, doc_file, doc_title, chapter, section, article,
                           article_heading, clause, point, chunk_index, content,
                           word_count, chunk_type
                    FROM chunks
                    ORDER BY chunk_id
                    """
                )
                rows = cur.fetchall()
                cols = [
                    'chunk_id', 'doc_file', 'doc_title', 'chapter', 'section', 'article',
                    'article_heading', 'clause', 'point', 'chunk_index', 'content',
                    'word_count', 'chunk_type'
                ]
                chunks = [dict(zip(cols, row)) for row in rows]
                conn.close()
            except Exception as e:
                print(f"‚ö†Ô∏è  L·ªói ƒë·ªçc SQLite ({e}).")
                chunks = None
    # Kh√¥ng c√≤n fallback JSON
    if chunks is None:
        print("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu chunks trong Parquet/SQLite!")
        return None, None

    print(f"üìä T·ªïng s·ªë chunks: {len(chunks)}")

    # L·∫•y n·ªôi dung chunks v√† ids ·ªïn ƒë·ªãnh
    # Gi·ªØ '_' theo ENV ƒë·ªÉ b·∫£o to√†n c·ª•m t·ª´ gh√©p c·ªßa PyVi khi embedding
    keep_underscore = os.getenv("LEGALADVISOR_EMB_KEEP_UNDERSCORE", "1").lower() in ("1", "true", "yes", "on")

    def normalize_for_embedding(text: str) -> str:
        _t = (text or '').strip()
        if keep_underscore:
            return _t
        return _t.replace('_', ' ')

    # Gi·∫£m k√≠ch th∆∞·ªõc b·∫±ng c√°ch c·∫Øt content input embedding (v√≠ d·ª• 800 tokens ~ 4k chars)
    texts = [normalize_for_embedding((chunk.get('content', '') or '')[:4000]) for chunk in chunks]
    ids = [int(chunk.get('chunk_id')) if chunk.get('chunk_id') is not None else idx for idx, chunk in enumerate(chunks)]
    metadata = [{
        'chunk_id': chunk.get('chunk_id'),
        'doc_file': chunk.get('doc_file'),
        # Tr√°nh ph√¨nh metadata.json: c·∫Øt ti√™u ƒë·ªÅ t·ªëi ƒëa 200 k√Ω t·ª±
        'doc_title': (chunk.get('doc_title')[:200] if isinstance(chunk.get('doc_title'), str) else chunk.get('doc_title')),
        'chunk_index': chunk.get('chunk_index'),
        'word_count': chunk.get('word_count'),
        'chapter': chunk.get('chapter'),
        'section': chunk.get('section'),
        'article': chunk.get('article'),
        'article_heading': chunk.get('article_heading'),
        'clause': chunk.get('clause'),
        'point': chunk.get('point'),
        'chunk_type': chunk.get('chunk_type'),
        # Preview ng·∫Øn ƒë·ªÉ gi·ªØ dung l∆∞·ª£ng
        'preview': normalize_for_embedding(chunk.get('content', ''))[:200]
    } for chunk in chunks]

    return texts, metadata, ids

def create_embeddings(texts, model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
    """T·∫°o embeddings cho texts"""

    print(f"ü§ñ Load model: {model_name}")

    # Load model
    use_gpu_env = os.getenv("LEGALADVISOR_USE_GPU", "auto").lower()
    if use_gpu_env == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    elif use_gpu_env in ("1", "true", "yes", "on", "cuda", "gpu"):
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = "cpu"

    # Cho ph√©p override model qua ENV
    env_model = os.getenv("LEGALADVISOR_EMB_MODEL")
    if env_model:
        model_name = env_model

    print(f"üñ•Ô∏è  Device: {device}")
    print(f"üß† Embedding model: {model_name}")

    # ∆Øu ti√™n CLS pooling cho Sup-SimCSE n·∫øu ph√°t hi·ªán model t∆∞∆°ng ·ª©ng ho·∫∑c ENV y√™u c·∫ßu
    force_cls = os.getenv("LEGALADVISOR_EMB_POOLING", "").lower() == "cls"
    try_cls = ("sup-simcse" in model_name.lower()) or force_cls

    if try_cls:
        try:
            transformer = st_models.Transformer(model_name)
            pooling = st_models.Pooling(
                transformer.get_word_embedding_dimension(),
                pooling_mode_cls_token=True,
                pooling_mode_mean_tokens=False,
                pooling_mode_max_tokens=False,
            )
            model = SentenceTransformer(modules=[transformer, pooling], device=device)
            print("üîß Pooling: CLS (theo Sup-SimCSE)")
        except Exception as e:
            print(f"‚ÑπÔ∏è  Kh√¥ng th·ªÉ kh·ªüi t·∫°o CLS pooling ({e}). D√πng m·∫∑c ƒë·ªãnh c·ªßa SentenceTransformers (mean)")
            model = SentenceTransformer(model_name, device=device)
            print("üîß Pooling: mean (m·∫∑c ƒë·ªãnh)")
    else:
        model = SentenceTransformer(model_name, device=device)
        print("üîß Pooling: mean (m·∫∑c ƒë·ªãnh)")

    print("üîÑ T·∫°o embeddings...")

    # T·∫°o embeddings theo batch ƒë·ªÉ tr√°nh memory error
    batch_size = int(os.getenv("LEGALADVISOR_EMB_BATCH", "256"))
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Creating embeddings"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, normalize_embeddings=False)
        embeddings.append(batch_embeddings)

    # Gh√©p t·∫•t c·∫£ embeddings
    embeddings = np.vstack(embeddings)

    print(f"üìä Embeddings shape: {embeddings.shape}")

    return embeddings, model

def build_faiss_index(embeddings, ids=None):
    """Build FAISS index t·ª´ embeddings.

    N·∫øu cung c·∫•p ids (chunk_id), s·∫Ω s·ª≠ d·ª•ng IndexIDMap ƒë·ªÉ √°nh x·∫° ·ªïn ƒë·ªãnh.
    H·ªó tr·ª£ HNSW qua ENV LEGALADVISOR_FAISS_HNSW=1.
    """

    print("üèóÔ∏è X√¢y d·ª±ng FAISS index...")

    # L·∫•y dimension c·ªßa embeddings
    dimension = embeddings.shape[1]

    # Normalize embeddings ƒë·ªÉ d√πng cosine tr√™n h√¨nh c·∫ßu ƒë∆°n v·ªã
    faiss.normalize_L2(embeddings)

    use_hnsw = os.getenv("LEGALADVISOR_FAISS_HNSW", "0").lower() in ("1", "true", "yes", "on")
    metric_type = "ip"

    if use_hnsw:
        M = int(os.getenv("LEGALADVISOR_HNSW_M", "32"))
        try:
            # Th·ª≠ t·∫°o HNSW v·ªõi Inner Product (n·∫øu b·∫£n FAISS h·ªó tr·ª£)
            base_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)  # type: ignore
            metric_type = "hnsw_ip"
        except Exception:
            # Fallback: HNSW L2
            base_index = faiss.IndexHNSWFlat(dimension, M)
            metric_type = "hnsw_l2"
        # Thi·∫øt l·∫≠p tham s·ªë t√¨m ki·∫øm/x√¢y d·ª±ng
        try:
            efc = int(os.getenv("LEGALADVISOR_HNSW_EF_CONSTRUCTION", "200"))
            efs = int(os.getenv("LEGALADVISOR_HNSW_EF_SEARCH", "64"))
            base_index.hnsw.efConstruction = efc
            base_index.hnsw.efSearch = efs
        except Exception:
            pass
    else:
        # D√πng IndexFlatIP cho cosine similarity
        base_index = faiss.IndexFlatIP(dimension)
        metric_type = "ip"

    # Add vectors (b·ªçc IDMap n·∫øu c√≥ ids)
    if ids is not None:
        index = faiss.IndexIDMap(base_index)
        ids_array = np.asarray(ids, dtype=np.int64)
        index.add_with_ids(embeddings, ids_array)
    else:
        index = base_index
        index.add(embeddings)

    print(f"‚úÖ FAISS index created with {index.ntotal} vectors | type={metric_type}")

    return index, metric_type

def save_index_and_metadata(index, metadata, model, output_dir="../models/retrieval", used_id_map=True, metric_type: str = "ip"):
    """L∆∞u FAISS index v√† metadata"""

    # Lu√¥n l∆∞u v·ªÅ th∆∞ m·ª•c models/retrieval t·∫°i g·ªëc d·ª± √°n
    project_root = get_project_root()
    output_dir = project_root / "models" / "retrieval"
    output_dir.mkdir(parents=True, exist_ok=True)

    # L∆∞u FAISS index
    index_path = output_dir / "faiss_index.bin"
    faiss.write_index(index, str(index_path))
    print(f"üíæ FAISS index saved: {index_path}")

    # L∆∞u metadata
    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"üíæ Metadata saved: {metadata_path}")

    # L∆∞u model info
    # Th√™m th√¥ng tin pooling ƒë·ªÉ launcher hi·ªÉn th·ªã
    pooling = "mean"
    try:
        for m in getattr(model, 'modules', []):
            cls_name = m.__class__.__name__.lower()
            if 'pooling' in cls_name:
                if getattr(m, 'pooling_mode_cls_token', False):
                    pooling = 'cls'
                elif getattr(m, 'pooling_mode_mean_tokens', False):
                    pooling = 'mean'
                elif getattr(m, 'pooling_mode_max_tokens', False):
                    pooling = 'max'
                break
    except Exception:
        pass

    model_info = {
        "model_name": getattr(model, "model_card", {}).get("name", None) or getattr(model, "_model_card", None) or os.getenv("LEGALADVISOR_EMB_MODEL") or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "embedding_dim": index.d,
        "num_chunks": index.ntotal,
        "uses_id_map": bool(used_id_map),
        "metric_type": metric_type,
        "pooling": pooling
    }

    model_info_path = output_dir / "model_info.json"
    with open(model_info_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    print(f"üíæ Model info saved: {model_info_path}")

def main():
    """H√†m ch√≠nh"""

    print("üöÄ B·∫Øt ƒë·∫ßu t·∫°o FAISS index cho retrieval...")

    # Load document chunks
    loaded = load_document_chunks()
    if loaded is None or loaded[0] is None:
        return
    texts, metadata, ids = loaded

    # T·∫°o embeddings
    embeddings, model = create_embeddings(texts)

    # Build FAISS index
    index, metric_type = build_faiss_index(embeddings, ids=ids)

    # L∆∞u index v√† metadata
    save_index_and_metadata(index, metadata, model, used_id_map=True, metric_type=metric_type)

    print("\n‚úÖ Ho√†n th√†nh t·∫°o FAISS index!")
    print("üìÅ C√°c file ƒë∆∞·ª£c l∆∞u t·∫°i: ../models/retrieval/")
    print("   - faiss_index.bin: FAISS index")
    print("   - metadata.json: Th√¥ng tin chunks")
    print("   - model_info.json: Th√¥ng tin model")

if __name__ == "__main__":
    main()
