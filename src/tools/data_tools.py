#!/usr/bin/env python3
"""
CLI h·ª£p nh·∫•t cho c√°c t√°c v·ª• d·ªØ li·ªáu c·ªßa LegalAdvisor.

C√°c l·ªánh h·ªó tr·ª£:
- download-viquad: T·∫£i (ho·∫∑c t·∫°o mock) b·ªô d·ªØ li·ªáu ViQuAD
- split-chunks: T·∫°o split IDs cho retrieval chunks t·ª´ SQLite/Parquet (th√≠ nghi·ªám)
- export-txt: Xu·∫•t txt t·ª´ SQLite/Parquet ƒë·ªÉ debug/tham kh·∫£o
"""

import argparse
import json
from pathlib import Path
from typing import List

from ..utils.paths import get_project_root, get_processed_data_dir


def cmd_download_viquad() -> bool:
    """T·∫£i ViQuAD v·ªÅ data/raw/ViQuAD (ho·∫∑c t·∫°o mock n·∫øu l·ªói)."""
    base = get_project_root() / 'data' / 'raw' / 'ViQuAD'
    base.mkdir(parents=True, exist_ok=True)

    existing = [base / 'train.json', base / 'validation.json', base / 'test.json']
    if all(p.exists() for p in existing):
        print("‚úÖ Dataset ViQuAD ƒë√£ c√≥ s·∫µn.")
        return True

    print("üöÄ ƒêang t·∫£i ViQuAD t·ª´ Hugging Face...")
    try:
        from datasets import load_dataset  # type: ignore
        dataset = load_dataset("bigscience-data/roots_ca_viquiquad")
        for split in ['train', 'validation', 'test']:
            if split in dataset:
                out = base / f"{split}.json"
                with open(out, 'w', encoding='utf-8') as f:
                    json.dump([dict(it) for it in dataset[split]], f, ensure_ascii=False, indent=2)
                print(f"üíæ L∆∞u {split}: {out}")
        print("‚úÖ T·∫£i ViQuAD th√†nh c√¥ng!")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i t·ª´ Hugging Face ({e}). T·∫°o mock dataset...")

    mock = {
        "train": [
            {
                "context": "ƒêi·ªÅu 1... M·ªçi ng∆∞·ªùi ƒë·ªÅu b√¨nh ƒë·∫≥ng v·ªÅ quy·ªÅn l·ª£i v√† nghƒ©a v·ª• c√¥ng d√¢n...",
                "question": "M·ªçi ng∆∞·ªùi ƒë·ªÅu b√¨nh ƒë·∫≥ng v·ªÅ ƒëi·ªÅu g√¨?",
                "answers": {"text": ["quy·ªÅn l·ª£i v√† nghƒ©a v·ª• c√¥ng d√¢n"], "answer_start": [15]},
            },
        ] * 50,
        "validation": [
            {
                "context": "ƒêi·ªÅu 3... Quy·ªÅn con ng∆∞·ªùi, quy·ªÅn c√¥ng d√¢n ch·ªâ c√≥ th·ªÉ b·ªã h·∫°n ch·∫ø...",
                "question": "Quy·ªÅn con ng∆∞·ªùi c√≥ th·ªÉ b·ªã h·∫°n ch·∫ø khi n√†o?",
                "answers": {"text": ["theo quy ƒë·ªãnh c·ªßa lu·∫≠t trong tr∆∞·ªùng h·ª£p c·∫ßn thi·∫øt"], "answer_start": [58]},
            },
        ] * 10,
        "test": [
            {
                "context": "ƒêi·ªÅu 4... quy·ªÅn b·∫•t kh·∫£ x√¢m ph·∫°m v·ªÅ th√¢n th·ªÉ...",
                "question": "M·ªçi ng∆∞·ªùi c√≥ nh·ªØng quy·ªÅn g√¨ v·ªÅ th√¢n th·ªÉ?",
                "answers": {"text": ["quy·ªÅn s·ªëng, quy·ªÅn b·∫•t kh·∫£ x√¢m ph·∫°m v·ªÅ th√¢n th·ªÉ"], "answer_start": [15]},
            },
        ] * 10,
    }

    for split in ['train', 'validation', 'test']:
        out = base / f"{split}.json"
        with open(out, 'w', encoding='utf-8') as f:
            json.dump(mock[split], f, ensure_ascii=False, indent=2)
        print(f"üíæ L∆∞u mock {split}: {out}")
    print("‚úÖ ƒê√£ t·∫°o mock ViQuAD!")
    return True


def cmd_split_chunks(limit: int = 200000) -> bool:
    """T·∫°o split IDs cho chunks t·ª´ SQLite/Parquet (ch·ªâ ph·ª•c v·ª• th√≠ nghi·ªám)."""
    from sklearn.model_selection import train_test_split  # type: ignore

    processed = get_processed_data_dir()
    sqlite_path = processed / 'smart_chunks_stable.db'
    parquet_path = processed / 'smart_chunks_stable.parquet'

    rows: List[int] = []
    try:
        if sqlite_path.exists():
            import sqlite3
            conn = sqlite3.connect(str(sqlite_path))
            cur = conn.cursor()
            cur.execute("SELECT chunk_id FROM chunks ORDER BY chunk_id LIMIT ?", (int(limit),))
            rows = [r[0] for r in cur.fetchall()]
            conn.close()
        elif parquet_path.exists():
            import pandas as pd  # type: ignore
            df = pd.read_parquet(parquet_path)
            rows = df.sort_values('chunk_id').head(limit)['chunk_id'].astype(int).tolist()
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y SQLite/Parquet ƒë·ªÉ split chunks")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc processed data: {e}")
        return False

    print(f"üìä T·ªïng s·ªë chunk IDs (gi·ªõi h·∫°n): {len(rows)}")

    train_ids, temp_ids = train_test_split(rows, test_size=0.2, random_state=42)
    val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)

    out_dir = processed / 'splits'
    out_dir.mkdir(exist_ok=True)
    for name, ids in {
        'train_ids': train_ids,
        'validation_ids': val_ids,
        'test_ids': test_ids,
    }.items():
        out = out_dir / f"{name}.json"
        with open(out, 'w', encoding='utf-8') as f:
            json.dump(ids, f, ensure_ascii=False, indent=2)
        print(f"üíæ L∆∞u {name}: {len(ids)} ‚Üí {out}")

    print("‚úÖ Ho√†n th√†nh split chunks IDs!")
    return True


def cmd_export_txt(limit: int = 2000) -> bool:
    """Xu·∫•t txt t·ª´ processed data, nh√≥m theo doc_file (debug)."""
    processed = get_processed_data_dir()
    sqlite_path = processed / 'smart_chunks_stable.db'
    parquet_path = processed / 'smart_chunks_stable.parquet'
    out_dir = get_project_root() / 'data' / 'txt_documents'
    out_dir.mkdir(parents=True, exist_ok=True)

    rows = []
    try:
        if sqlite_path.exists():
            import sqlite3
            conn = sqlite3.connect(str(sqlite_path))
            cur = conn.cursor()
            cur.execute("SELECT doc_file, content FROM chunks ORDER BY chunk_id LIMIT ?", (int(limit),))
            rows = cur.fetchall()
            conn.close()
        elif parquet_path.exists():
            import pandas as pd  # type: ignore
            df = pd.read_parquet(parquet_path)
            df = df.sort_values('chunk_id').head(limit)
            rows = list(zip(df['doc_file'].tolist(), df['content'].tolist()))
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y SQLite/Parquet")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc processed data: {e}")
        return False

    from collections import defaultdict
    grouped = defaultdict(list)
    for doc_file, content in rows:
        name = Path(doc_file).name if doc_file else 'unknown'
        text = (content or '').replace('_', ' ').strip()
        if text:
            grouped[name].append(text)

    total = 0
    for name, lines in grouped.items():
        try:
            with open(out_dir / f"{name}.txt", 'w', encoding='utf-8') as f:
                f.write('\n\n'.join(lines))
            total += 1
        except Exception:
            pass

    print(f"‚úÖ ƒê√£ xu·∫•t {total} file v√†o {out_dir}")
    return True


def main():
    parser = argparse.ArgumentParser(description="LegalAdvisor Data CLI")
    sub = parser.add_subparsers(dest='command', required=True)

    sub.add_parser('download-viquad', help='T·∫£i ViQuAD ho·∫∑c t·∫°o mock n·∫øu l·ªói')

    sp_split = sub.add_parser('split-chunks', help='T·∫°o split IDs cho chunks (th√≠ nghi·ªám)')
    sp_split.add_argument('--limit', type=int, default=200000, help='Gi·ªõi h·∫°n s·ªë chunk ƒë·ªçc')

    sp_export = sub.add_parser('export-txt', help='Xu·∫•t txt t·ª´ processed data ƒë·ªÉ debug')
    sp_export.add_argument('--limit', type=int, default=2000, help='Gi·ªõi h·∫°n s·ªë chunk xu·∫•t')

    args = parser.parse_args()

    if args.command == 'download-viquad':
        ok = cmd_download_viquad()
    elif args.command == 'split-chunks':
        ok = cmd_split_chunks(limit=args.limit)
    elif args.command == 'export-txt':
        ok = cmd_export_txt(limit=args.limit)
    else:
        ok = False

    if not ok:
        raise SystemExit(1)


if __name__ == '__main__':
    main()


